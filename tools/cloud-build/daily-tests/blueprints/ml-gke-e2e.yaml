# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---

blueprint_name: ml-gke-e2e

vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: ml-gke-e2e
  region: asia-southeast1
  zones:
  - asia-southeast1-b  # g2 machine has better availability in this zone

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr: <your-ip-address>/32

deployment_groups:
- group: primary
  modules:
  - id: network1
    source: modules/network/vpc
    settings:
      subnetwork_name: gke-subnet1
      secondary_ranges:
        gke-subnet1:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: gke_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectViewer
      - artifactregistry.reader

  - id: gke_cluster
    source: modules/scheduler/gke-cluster
    use: [network1, gke_service_account]
    settings:
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      master_authorized_networks:
      - display_name: deployment-machine
        cidr_block: $(vars.authorized_cidr)
    outputs: [instructions]

  - id: g2_latest_driver
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gke_service_account]
    settings:
      name: g2-latest-driver
      machine_type: g2-standard-4
      guest_accelerator:
      - gpu_driver_installation_config:
          gpu_driver_version: "LATEST"
        gpu_sharing_config:
          max_shared_clients_per_gpu: 2
          gpu_sharing_strategy: "MPS"

  - id: job_template_g2_latest_driver
    source: modules/compute/gke-job-template
    use: [g2_latest_driver]
    settings:
      name: job-g2-latest-driver
      image: nvidia/cuda:11.0.3-runtime-ubuntu20.04
      command:
      - nvidia-smi
      node_count: 1
      node_selectors: [
        {
          "key": "cloud.google.com/gke-nodepool",
          "value": "g2-latest-driver"
        }
      ]
    outputs: [instructions]

  - id: n1_pool_default
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gke_service_account]
    settings:
      name: n1-pool-default
      disk_type: pd-balanced
      machine_type: n1-standard-4
      guest_accelerator:
      - type: nvidia-tesla-t4
        count: 2

  - id: job_template_n1_pool_default
    source: modules/compute/gke-job-template
    use: [n1_pool_default]
    settings:
      name: job-n1-pool-default
      image: nvidia/cuda:11.0.3-runtime-ubuntu20.04
      command:
      - nvidia-smi
      node_count: 1
      node_selectors: [
        {
          "key": "cloud.google.com/gke-nodepool",
          "value": "n1-pool-default"
        }
      ]
    outputs: [instructions]

  - id: n1_pool_full_spec
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gke_service_account]
    settings:
      name: n1-pool-full-spec
      disk_type: pd-balanced
      machine_type: n1-standard-4
      guest_accelerator:
      - type: nvidia-tesla-t4
        count: 2
        gpu_driver_installation_config:
          gpu_driver_version: "LATEST"
        gpu_sharing_config:
          max_shared_clients_per_gpu: 2
          gpu_sharing_strategy: "TIME_SHARING"

  - id: job_template_n1_pool_full_spec
    source: modules/compute/gke-job-template
    use: [n1_pool_full_spec]
    settings:
      name: job-n1-pool-full-spec
      image: nvidia/cuda:11.0.3-runtime-ubuntu20.04
      command:
      - nvidia-smi
      node_count: 1
      node_selectors: [
        {
          "key": "cloud.google.com/gke-nodepool",
          "value": "n1-pool-full-spec"
        }
      ]
    outputs: [instructions]

  - id: default_settings_pool
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gke_service_account]
    settings:
      name: default-settings-pool

  - id: job_default_settings_pool
    source: modules/compute/gke-job-template
    use: [default_settings_pool]
    settings:
      name: job-default-settings-pool
      image: busybox
      command:
      - echo
      - Hello World
      node_count: 1
      node_selectors: [
        {
          "key": "cloud.google.com/gke-nodepool",
          "value": "default-settings-pool"
        }
      ]
    outputs: [instructions]
