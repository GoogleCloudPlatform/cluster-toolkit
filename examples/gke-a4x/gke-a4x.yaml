# Copyright 2025 "Google LLC"
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: gke-a4x

vars:

  # The following variables should be over-written in the deployment.yaml file.
  # Your GCP Project ID
  project_id:

  # This should be unique across all of your Cluster
  # Toolkit Deployments.
  deployment_name: gke-a4x

  # The GCP Region used for this deployment.
  region:

  # The GCP Zone used for this deployment.
  zone:

  # Cidr block containing the IP of the machine calling terraform.
  # To allow all (IAM restrictions still enforced), use 0.0.0.0/0
  # To allow only your IP address, use <YOUR-IP-ADDRESS>/32
  authorized_cidr:

  # The number of nodes to be created
  static_node_count:

  system_node_pool_disk_size_gb: 200
  a4x_node_pool_disk_size_gb: 100
  k8s_service_account_name: workload-identity-k8s-sa

  # The name of the compute engine reservation of A4X nodes in the form of
  # <project>/<reservation-name>
  # To target a BLOCK_NAME, the name of the extended reservation
  # can be inputted as <reservation-name>/reservationBlocks/<reservation-block-name>
  reservation:
  compute_nodepool_machine_type: a4x-highgpu-4g
  system_nodepool_machine_type: e2-standard-16

  # Installs NCCL library and Google NCCL plugin
  # Runs an init container on all GB200 GPU nodes with the NCCL plugin image
  gib_installer_path: $(ghpc_stage("./nccl-installer.yaml.tftpl"))
  nvidia_dra_driver_path: $(ghpc_stage("./nvidia-dra-driver.yaml"))
  kueue_configuration_path: $(ghpc_stage("./kueue-configuration.yaml.tftpl"))
  accelerator_type: nvidia-gb200
  version_prefix: "1.33."

  # # To enable Managed-Lustre please uncomment this section and fill out the settings.
  # # Additionally, please uncomment the private_service_access, lustre_firewall_rule, managed-lustre and lustre-pv modules.
  # # Managed Lustre is only supported in specific regions and zones
  # # Please refer https://cloud.google.com/managed-lustre/docs/locations

  # # Managed-Lustre instance name. This should be unique for each deployment.
  # lustre_instance_id: $(vars.deployment_name)

  # # The values of size_gib and per_unit_storage_throughput are co-related
  # # Please refer https://cloud.google.com/managed-lustre/docs/create-instance#performance-tiers
  # # Storage capacity of the lustre instance in GiB
  # lustre_size_gib: 36000

  # # Maximum throughput of the lustre instance in MBps per TiB
  # per_unit_storage_throughput: 500

terraform_providers:
  google:
    source: hashicorp/google
    version: 7.2.0
    configuration:
      project: $(vars.project_id)
      region: $(vars.region)
      zone: $(vars.zone)
      compute_custom_endpoint: "https://www.googleapis.com/compute/beta/"

  google-beta:
    source: hashicorp/google-beta
    version: 7.2.0
    configuration:
      project: $(vars.project_id)
      region: $(vars.region)
      zone: $(vars.zone)
      compute_custom_endpoint: "https://www.googleapis.com/compute/beta/"


deployment_groups:
- group: primary
  modules:
  - id: gke-a4x-net-0
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-0
      ips_per_nat: 6
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-0
        subnet_region: $(vars.region)
        subnet_ip: 192.168.0.0/18
      secondary_ranges_list:
      - subnetwork_name: $(vars.deployment_name)-sub-0
        ranges:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20
      firewall_rules:
      - name: $(vars.deployment_name)-internal-0
        ranges: [192.168.0.0/16]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: gke-a4x-net-1
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-1
      ips_per_nat: 6
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-1
        subnet_region: $(vars.region)
        subnet_ip: 192.168.64.0/18
      firewall_rules:
      - name: $(vars.deployment_name)-internal-1
        ranges: [192.168.0.0/16]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: gke-a4x-rdma-net
    source: modules/network/gpu-rdma-vpc
    settings:
      network_name: $(vars.deployment_name)-rdma-net
      network_profile: https://www.googleapis.com/compute/beta/projects/$(vars.project_id)/global/networkProfiles/$(vars.zone)-vpc-roce
      network_routing_mode: REGIONAL
      subnetworks_template:
        name_prefix: $(vars.deployment_name)-rdma-sub
        count: 4
        ip_range: 192.168.128.0/18
        region: $(vars.region)

  - id: node_pool_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-np-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectViewer
      - artifactregistry.reader

  - id: workload_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-wl-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectAdmin
      - artifactregistry.reader

  - id: a4x-cluster
    source: modules/scheduler/gke-cluster
    use: [gke-a4x-net-0, workload_service_account]
    settings:
      system_node_pool_machine_type: $(vars.system_nodepool_machine_type)
      system_node_pool_disk_size_gb: $(vars.system_node_pool_disk_size_gb)
      system_node_pool_taints: []
      enable_dcgm_monitoring: true
      enable_gcsfuse_csi: true
      enable_managed_lustre_csi: true # Enable Managed Lustre for the cluster
      enable_k8s_beta_apis:
      - "resource.k8s.io/v1beta1/deviceclasses"
      - "resource.k8s.io/v1beta1/resourceclaims"
      - "resource.k8s.io/v1beta1/resourceclaimtemplates"
      - "resource.k8s.io/v1beta1/resourceslices"
      enable_private_endpoint: false  # Allows access from authorized public IPs
      configure_workload_identity_sa: true
      k8s_service_account_name: $(vars.k8s_service_account_name)
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr)  # Allows your machine to run the kubectl command. Required for multi network setup.
        display_name: "kubectl-access-network"
      version_prefix: $(vars.version_prefix)
      release_channel: RAPID
      maintenance_exclusions:
      - name: no-minor-or-node-upgrades-indefinite
        start_time: "2025-04-01T00:00:00Z"
        end_time: "2026-04-10T00:00:00Z"
        exclusion_scope: NO_MINOR_OR_NODE_UPGRADES
      additional_networks:
        $(concat(
          [{
            network=gke-a4x-net-1.network_name,
            subnetwork=gke-a4x-net-1.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
         gke-a4x-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  # # --- MANAGED LUSTRE ADDITIONS ---
  # # Private Service Access (PSA) requires the compute.networkAdmin role which is
  # # included in the Owner role, but not Editor.
  # # PSA is required for all Managed Lustre functionality.
  # # https://cloud.google.com/vpc/docs/configure-private-services-access#permissions
  # - id: private_service_access
  #   source: community/modules/network/private-service-access
  #   use: [gke-a4x-net-0]
  #   settings:
  #     prefix_length: 24

  # # Firewall to allow Managed Lustre connection
  # - id: lustre_firewall_rule
  #   source: modules/network/firewall-rules
  #   use: [gke-a4x-net-0]
  #   settings:
  #     ingress_rules:
  #     - name: $(vars.deployment_name)-allow-lustre-traffic
  #       description: Allow Managed Lustre traffic
  #       source_ranges:
  #       - $(private_service_access.cidr_range)
  #       allow:
  #       - protocol: tcp
  #       ports:
  #       - "988"

  # - id: managed-lustre
  #   source: modules/file-system/managed-lustre
  #   use: [gke-a4x-net-0, private_service_access]
  #   settings:
  #     name: $(vars.lustre_instance_id)
  #     local_mount: /lustre
  #     remote_mount: lustrefs
  #     size_gib: $(vars.lustre_size_gib)
  #     per_unit_storage_throughput: $(vars.per_unit_storage_throughput)

  # - id: lustre-pv
  #   source: modules/file-system/gke-persistent-volume
  #   use: [managed-lustre, a4x-cluster]
  #   settings:
  #     capacity_gib: $(vars.lustre_size_gib)

  - id: workload_policy
    source: modules/compute/resource-policy
    settings:
      name: "a4x-workload-policy"
      project_id: $(vars.project_id)
      region: $(vars.region)
      workload_policy:
        accelerator_topology: "1x72"
        type: "HIGH_THROUGHPUT"

  - id: a4x-pool
    source: modules/compute/gke-node-pool
    use: [a4x-cluster, node_pool_service_account, workload_policy]
    settings:
      machine_type: $(vars.compute_nodepool_machine_type)
      auto_upgrade: true
      zones: [$(vars.zone)]
      disk_size_gb: $(vars.a4x_node_pool_disk_size_gb)
      static_node_count: $(vars.static_node_count)
      disk_type: hyperdisk-balanced
      guest_accelerator:
      - type: nvidia-gb200
        count: 4
      reservation_affinity:
        consume_reservation_type: SPECIFIC_RESERVATION
        specific_reservations:
        - name: $(vars.reservation)
      additional_networks:
        $(concat(
          [{
            network=gke-a4x-net-1.network_name,
            subnetwork=gke-a4x-net-1.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
         gke-a4x-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: workload-manager-install
    source: modules/management/kubectl-apply
    use: [a4x-cluster]
    settings:
      target_architecture: "arm64"
      kueue:
        install: true
        config_path: $(vars.kueue_configuration_path)
        config_template_vars:
          num_gpus: $(a4x-pool.static_gpu_count)
          accelerator_type: $(vars.accelerator_type)
      jobset:
        install: true
      nvidia_dra_driver:
        install: true
      gib:
        install: true  # NCCL gIB plugin via DaemonSet initContainer
        path: $(vars.gib_installer_path)
        template_vars:
          image: "us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-arm64"
          version: v1.0.7
          accelerator_count: 4
          node_affinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: cloud.google.com/gke-accelerator
                  operator: In
                  values:
                  - $(vars.accelerator_type)
                - key: kubernetes.io/arch
                  operator: In
                  values:
                  - arm64
      apply_manifests:
      - source: $(vars.nvidia_dra_driver_path)

  - id: job-template
    source: modules/compute/gke-job-template
    use: [a4x-pool]
    settings:
      image: nvidia/cuda:12.8.0-runtime-ubuntu24.04
      command:
      - nvidia-smi
      node_count: $(vars.static_node_count)
      name: run-nvidia-smi
      k8s_service_account_name: $(vars.k8s_service_account_name)
      tolerations:
      - key: kubernetes.io/arch
        operator: Equal
        value: arm64
        effect: NoSchedule
    outputs: [instructions]
