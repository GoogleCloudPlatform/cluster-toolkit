# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
blueprint_name: gke-managed-hyperdisk
vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: gke-managed-hyperdisk
  region: europe-west1
  zone: europe-west1-b

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr: <your-ip-address>/32

deployment_groups:
- group: primary
  modules:
  - id: network
    source: modules/network/vpc
    settings:
      subnetwork_name: gke-subnet-hyperdisk
      secondary_ranges:
        gke-subnet-hyperdisk:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: node_pool_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-np-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectViewer
      - artifactregistry.reader

  - id: workload_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-wl-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectAdmin
      - artifactregistry.reader

  - id: gke_cluster
    source: modules/scheduler/gke-cluster
    use: [network, workload_service_account]
    settings:
      release_channel: RAPID
      enable_persistent_disk_csi: true # enable Hyperdisk for the cluster
      configure_workload_identity_sa: true
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      master_authorized_networks:
      - display_name: deployment-machine
        cidr_block: $(vars.authorized_cidr)
      maintenance_exclusions:
      - name: no-minor-or-node-upgrades-indefinite
        start_time: "2024-12-01T00:00:00Z"
        end_time: "2025-12-22T00:00:00Z"
        exclusion_scope: NO_MINOR_OR_NODE_UPGRADES
    outputs: [instructions]

  ### Set up storage class and persistent volume claim for Hyperdisk ###
  - id: hyperdisk-balanced-setup
    source: modules/file-system/gke-storage
    use: [gke_cluster]
    settings:
      storage_type: Hyperdisk-balanced
      access_mode: ReadWriteOnce
      sc_volume_binding_mode: Immediate
      sc_reclaim_policy: Delete
      sc_topology_zones: [$(vars.zone)]
      pvc_count: 1
      capacity_gb: 100

  - id: hyperdisk-throughput-setup
    source: modules/file-system/gke-storage
    use: [gke_cluster]
    settings:
      storage_type: Hyperdisk-throughput
      access_mode: ReadWriteOnce
      sc_volume_binding_mode: Immediate
      sc_reclaim_policy: Delete
      sc_topology_zones: [$(vars.zone)]
      pvc_count: 1
      capacity_gb: 5000

  - id: hyperdisk-extreme-setup
    source: modules/file-system/gke-storage
    use: [gke_cluster]
    settings:
      storage_type: Hyperdisk-extreme
      access_mode: ReadWriteOnce
      sc_volume_binding_mode: Immediate
      sc_reclaim_policy: Delete
      sc_topology_zones: [$(vars.zone)]
      pvc_count: 1
      capacity_gb: 100

  - id: sample-pool
    source: modules/compute/gke-node-pool
    use: [gke_cluster, node_pool_service_account]
    settings:
      name: sample-pool
      zones: [$(vars.zone)]
      machine_type: c3-standard-88 # Hyperdisk-extreme required C3 machine with 88 or more vCPUs
      auto_upgrade: true

  # Train a TensorFlow model with Keras and Hyperdisk Balanced on GKE
  # Tutorial: https://cloud.google.com/parallelstore/docs/tensorflow-sample
  - id: hyperdisk-balanced-job
    source: modules/compute/gke-job-template
    use:
    - gke_cluster
    - hyperdisk-balanced-setup
    settings:
      name: tensorflow
      image: jupyter/tensorflow-notebook@sha256:173f124f638efe870bb2b535e01a76a80a95217e66ed00751058c51c09d6d85d
      security_context:  # to make sure the job have enough access to execute the jobs and r/w from hyperdisk
      - key: runAsUser
        value: 1000
      - key: runAsGroup
        value: 100
      - key: fsGroup
        value: 100
      command:
      - bash
      - -c
      - |
        pip install transformers datasets
        python - <<EOF
        from datasets import load_dataset
        dataset = load_dataset("glue", "cola", cache_dir='/data/hyperdisk-balanced-pvc-0')
        dataset = dataset["train"]
        from transformers import AutoTokenizer
        import numpy as np
        tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
        tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
        tokenized_data = dict(tokenized_data)
        labels = np.array(dataset["label"])
        from transformers import TFAutoModelForSequenceClassification
        from tensorflow.keras.optimizers import Adam
        model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
        model.compile(optimizer=Adam(3e-5))
        model.fit(tokenized_data, labels)
        EOF
      node_count: 1
    outputs: [instructions]

  # Train a TensorFlow model with Keras and Hyperdisk Extreme on GKE
  # Tutorial: https://cloud.google.com/parallelstore/docs/tensorflow-sample
  - id: hyperdisk-extreme-job
    source: modules/compute/gke-job-template
    use:
    - gke_cluster
    - hyperdisk-extreme-setup
    settings:
      name: tensorflow
      image: jupyter/tensorflow-notebook@sha256:173f124f638efe870bb2b535e01a76a80a95217e66ed00751058c51c09d6d85d
      security_context:  # to make sure the job have enough access to execute the jobs and r/w from hyperdisk
      - key: runAsUser
        value: 1000
      - key: runAsGroup
        value: 100
      - key: fsGroup
        value: 100
      command:
      - bash
      - -c
      - |
        pip install transformers datasets
        python - <<EOF
        from datasets import load_dataset
        dataset = load_dataset("glue", "cola", cache_dir='/data/hyperdisk-extreme-pvc-0')
        dataset = dataset["train"]
        from transformers import AutoTokenizer
        import numpy as np
        tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
        tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
        tokenized_data = dict(tokenized_data)
        labels = np.array(dataset["label"])
        from transformers import TFAutoModelForSequenceClassification
        from tensorflow.keras.optimizers import Adam
        model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
        model.compile(optimizer=Adam(3e-5))
        model.fit(tokenized_data, labels)
        EOF
      node_count: 1
    outputs: [instructions]

  # Train a TensorFlow model with Keras and Hyperdisk Throughput on GKE
  # Tutorial: https://cloud.google.com/parallelstore/docs/tensorflow-sample
  - id: hyperdisk-throughput-job
    source: modules/compute/gke-job-template
    use:
    - gke_cluster
    - hyperdisk-throughput-setup
    settings:
      name: tensorflow
      image: jupyter/tensorflow-notebook@sha256:173f124f638efe870bb2b535e01a76a80a95217e66ed00751058c51c09d6d85d
      security_context:  # to make sure the job have enough access to execute the jobs and r/w from hyperdisk
      - key: runAsUser
        value: 1000
      - key: runAsGroup
        value: 100
      - key: fsGroup
        value: 100
      command:
      - bash
      - -c
      - |
        pip install transformers datasets
        python - <<EOF
        from datasets import load_dataset
        dataset = load_dataset("glue", "cola", cache_dir='/data/hyperdisk-throughput-pvc-0')
        dataset = dataset["train"]
        from transformers import AutoTokenizer
        import numpy as np
        tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
        tokenized_data = tokenizer(dataset["sentence"], return_tensors="np", padding=True)
        tokenized_data = dict(tokenized_data)
        labels = np.array(dataset["label"])
        from transformers import TFAutoModelForSequenceClassification
        from tensorflow.keras.optimizers import Adam
        model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
        model.compile(optimizer=Adam(3e-5))
        model.fit(tokenized_data, labels)
        EOF
      node_count: 1
    outputs: [instructions]
