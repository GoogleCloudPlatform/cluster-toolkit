# Copyright 2026 "Google LLC"
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: gke-a4x-max-bm

vars:

  # The following variables should be over-written in the deployment.yaml file.
  # Your GCP Project ID
  project_id:

  # This should be unique across all of your Cluster
  # Toolkit Deployments.
  deployment_name: gke-a4x-max-bm-dep

  # The GCP Region used for this deployment.
  region:

  # The GCP Zone used for this deployment.
  zone:

  # Cidr block containing the IP of the machine calling terraform.
  # To allow all (IAM restrictions still enforced), use 0.0.0.0/0
  # To allow only your IP address, use <YOUR-IP-ADDRESS>/32
  authorized_cidr:

  # The number of nodes to be created
  static_node_count:

  system_node_pool_disk_size_gb: 200
  a4x_max_node_pool_disk_size_gb: 100
  k8s_service_account_name: workload-identity-k8s-sa

  # The name of the compute engine reservation of A4X-Max nodes in the form of
  # <project>/<reservation-name>
  # To target a BLOCK_NAME, the name of the extended reservation
  # can be inputted as <reservation-name>/reservationBlocks/<reservation-block-name>
  reservation:
  compute_nodepool_machine_type: a4x-maxgpu-4g-metal
  system_nodepool_machine_type: e2-standard-16

  nvidia_dra_driver_path: $(ghpc_stage("./nvidia-dra-driver.yaml"))
  asapd_lite_installer_path: $(ghpc_stage("./asapd-lite-installer.yaml"))
  accelerator_type: nvidia-gb300
  version_prefix: "1.34."

deployment_groups:
- group: primary
  modules:
  - id: gke-a4x-max-net-0
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-0
      ips_per_nat: 6
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-0
        subnet_region: $(vars.region)
        subnet_ip: 192.168.0.0/18
      secondary_ranges_list:
      - subnetwork_name: $(vars.deployment_name)-sub-0
        ranges:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20
      firewall_rules:
      - name: $(vars.deployment_name)-internal-0
        ranges: [192.168.0.0/16]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: gke-a4x-max-net-1
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-1
      ips_per_nat: 6
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-1
        subnet_region: $(vars.region)
        subnet_ip: 192.168.64.0/18
      firewall_rules:
      - name: $(vars.deployment_name)-internal-1
        ranges: [192.168.0.0/16]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: gke-a4x-max-rdma-net
    source: modules/network/gpu-rdma-vpc
    settings:
      network_name: $(vars.deployment_name)-rdma-net
      network_profile: https://www.googleapis.com/compute/beta/projects/$(vars.project_id)/global/networkProfiles/$(vars.zone)-vpc-roce-metal
      network_routing_mode: REGIONAL
      subnetworks_template: null

  - id: node_pool_service_account
    source: modules/project/service-account
    settings:
      name: gke-np-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectViewer
      - artifactregistry.reader

  - id: workload_service_account
    source: modules/project/service-account
    settings:
      name: gke-wl-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectAdmin
      - artifactregistry.reader

  - id: a4x-max-cluster
    source: modules/scheduler/gke-cluster
    use: [gke-a4x-max-net-0, workload_service_account]
    settings:
      system_node_pool_machine_type: $(vars.system_nodepool_machine_type)
      system_node_pool_disk_size_gb: $(vars.system_node_pool_disk_size_gb)
      system_node_pool_taints: []
      enable_dataplane_v2: true
      enable_gcsfuse_csi: true
      enable_private_endpoint: false
      enable_shielded_nodes: false
      configure_workload_identity_sa: true
      k8s_service_account_name: $(vars.k8s_service_account_name)
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr)
        display_name: "kubectl-access-network"
      version_prefix: $(vars.version_prefix)
      release_channel: REGULAR
      maintenance_exclusions:
      - name: no-minor-or-node-upgrades-indefinite
        start_time: "2025-12-01T00:00:00Z"
        exclusion_scope: NO_MINOR_OR_NODE_UPGRADES
        exclusion_end_time_behavior: UNTIL_END_OF_SUPPORT
      additional_networks:
        $(concat(
          [{
            network = gke-a4x-max-net-1.network_name,
            subnetwork = gke-a4x-max-net-1.subnetwork_name,
            subnetwork_project = vars.project_id,
            nic_type = "IDPF",
            queue_count = null,
            network_ip = null,
            stack_type = null,
            access_config = [],
            ipv6_access_config = [],
            alias_ip_range = []
          }],
          gke-a4x-max-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: workload_policy
    source: modules/compute/resource-policy
    settings:
      name: "a4x-max-workload-policy"
      project_id: $(vars.project_id)
      region: $(vars.region)
      workload_policy:
        accelerator_topology: "1x72"
        type: "HIGH_THROUGHPUT"

  - id: a4x-max-pool
    source: modules/compute/gke-node-pool
    use: [a4x-max-cluster, node_pool_service_account, workload_policy]
    settings:
      machine_type: $(vars.compute_nodepool_machine_type)
      auto_upgrade: true
      zones: [$(vars.zone)]
      disk_size_gb: $(vars.a4x_max_node_pool_disk_size_gb)
      static_node_count: $(vars.static_node_count)
      disk_type: hyperdisk-balanced
      guest_accelerator:
      - type: $(vars.accelerator_type)
        count: 4
      reservation_affinity:
        consume_reservation_type: SPECIFIC_RESERVATION
        specific_reservations:
        - name: $(vars.reservation)
      kubernetes_labels:
        cloud.google.com/gke-networking-dra-driver: "true"
        cloud.google.com/gke-dpv2-unified-cni: "cni-migration"
      linux_node_config:
        hugepages_config:
          hugepage_size_2m: 4096
      additional_networks:
        $(concat(
          [{
            network = gke-a4x-max-net-1.network_name,
            subnetwork = gke-a4x-max-net-1.subnetwork_name,
            subnetwork_project = vars.project_id,
            nic_type = "IDPF",
            queue_count = null,
            network_ip = null,
            stack_type = null,
            access_config = [],
            ipv6_access_config = [],
            alias_ip_range = []
          }],
          gke-a4x-max-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: workload-manager-install
    source: modules/management/kubectl-apply
    use: [a4x-max-cluster]
    settings:
      target_architecture: "arm64"
      asapd_lite:
        install: true
        config_path: $(vars.asapd_lite_installer_path)
      jobset:
        install: true
      nvidia_dra_driver:
        install: true
        version: "v25.8.0"
        accelerator_type: $(vars.accelerator_type)
      apply_manifests:
      - source: $(vars.nvidia_dra_driver_path)

  - id: job-template
    source: modules/compute/gke-job-template
    use: [a4x-max-pool]
    settings:
      image: nvidia/cuda:13.0.0-base-ubuntu22.04
      command:
      - nvidia-smi
      node_count: $(vars.static_node_count)
      name: run-nvidia-smi
      k8s_service_account_name: $(vars.k8s_service_account_name)
      tolerations:
      - key: kubernetes.io/arch
        operator: Equal
        value: arm64
        effect: NoSchedule
    outputs: [instructions]

  - id: training_bucket
    source: modules/file-system/cloud-storage-bucket
    settings:
      local_mount: /training-data
      name_prefix: training
      random_suffix: true
      force_destroy: false
      enable_hierarchical_namespace: true

  - id: checkpoint_bucket
    source: modules/file-system/cloud-storage-bucket
    settings:
      local_mount: /checkpoint-data
      name_prefix: checkpoint
      random_suffix: true
      force_destroy: false
      enable_hierarchical_namespace: true

  # Create a remote mount of training_bucket using
  # mount options optimized for reading training data.
  # Based on Source of truth https://github.com/GoogleCloudPlatform/gcsfuse/blob/d1373b665b7f60e98856d2181f1193396ef16427/samples/gke-csi-yaml/gpu/checkpointing-pv.yaml#L15
  # Some of the options might be available only on latest GKE version, please check the cluster version to meet the required version https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-storage-fuse-csi-driver-perf
  - id: gcs-training
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(training_bucket.gcs_bucket_name)
      local_mount: /training-data
      fs_type: gcsfuse
      mount_options: "implicit-dirs,metadata-cache:ttl-secs:-1,metadata-cache:stat-cache-max-size-mb:-1,metadata-cache:type-cache-max-size-mb:-1,file-cache:max-size-mb:-1,file-cache:cache-file-for-range-read:true"

  # Create a remote mount of checkpoint_bucket using mount
  # options optimized for writing and reading checkpoint data.
  # Based on Source of truth https://github.com/GoogleCloudPlatform/gcsfuse/blob/d1373b665b7f60e98856d2181f1193396ef16427/samples/gke-csi-yaml/gpu/checkpointing-pv.yaml#L15
  # Some of the options might be available only on latest GKE version, please check the cluster version to meet the required version https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-storage-fuse-csi-driver-perf
  - id: gcs-checkpointing
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(checkpoint_bucket.gcs_bucket_name)
      local_mount: /checkpoint-data
      fs_type: gcsfuse
      mount_options: "implicit-dirs,metadata-cache:ttl-secs:-1,metadata-cache:stat-cache-max-size-mb:-1,metadata-cache:type-cache-max-size-mb:-1,file-cache:max-size-mb:-1,file-cache:cache-file-for-range-read:true,file-cache:enable-parallel-downloads:true,rename-dir-limit=200000"

  # Persistent Volume for training data
  - id: training-pv
    source: modules/file-system/gke-persistent-volume
    use: [gcs-training, a4x-max-cluster]
    settings:
      gcs_bucket_name: $(training_bucket.gcs_bucket_name)
      capacity_gib: 1000000

  # Persistent Volume for checkpoint data
  - id: checkpointing-pv
    source: modules/file-system/gke-persistent-volume
    use: [gcs-checkpointing, a4x-max-cluster]
    settings:
      gcs_bucket_name: $(checkpoint_bucket.gcs_bucket_name)
      capacity_gib: 1000000

  # This is an example job that will install and run an `fio`
  # benchmark against the training and checkpointing buckets.
  - id: fio-bench-job-template
    source: modules/compute/gke-job-template
    use: [checkpointing-pv, training-pv, a4x-max-pool]
    settings:
      security_context:  # to make sure the job have enough access to install the fio packages
      - key: runAsUser
        value: 0
      - key: runAsGroup
        value: 100
      - key: fsGroup
        value: 100
      # By adding an ephemeral volume, this will ensure that the job adds:
      # nodeSelector:
      #   cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
      # which is the best practice for using local-ssd for ephemeral storage.
      ephemeral_volumes:
      - type: local-ssd
        mount_path: /scratch-data
        size_gb: 1000  # Use 1 out of 12 TB for local scratch

      k8s_service_account_name: workload-identity-k8s-sa
      image: ubuntu:22.04
      tolerations:
      - key: kubernetes.io/arch
        operator: Equal
        value: arm64
        effect: NoSchedule

      command:
      - bash
      - -c
      - |

        set -eux
        export DEBIAN_FRONTEND=noninteractive

        # Install fio
        apt update -y && apt install -y fio

        # Use a tag to create a unique path for tests
        TAG=`date +%s`

        # Verify mountpoints
        df -h
        mountpoint /scratch-data
        mountpoint /checkpoint-data
        mountpoint /training-data

        # Create temporary directory for fio benchmarks
        mkdir -p /{scratch,training,checkpoint}-data/fio-benchmarks-${TAG}

        # The following will take roughly 10 minutes to complete

        # Perform scratch data write performance test
        fio --ioengine=libaio --filesize=10G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/scratch-data/fio-benchmarks-${TAG} \
          --name=scratch --blocksize=100m --iodepth=64 --readwrite=write

        # Perform training data reading performance test
        fio --ioengine=libaio --filesize=1G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/training-data/fio-benchmarks-${TAG} \
          --name=training --blocksize=1m --iodepth=64 --readwrite=randread

        # Perform checkpoint data writing performance test
        fio --ioengine=libaio --filesize=10G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/checkpoint-data/fio-benchmarks-${TAG} \
          --name=checkpoint --blocksize=100m --iodepth=64 --readwrite=write

        # Perform checkpoint data reading performance test
        fio --ioengine=libaio --filesize=10G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/checkpoint-data/fio-benchmarks-${TAG} \
          --name=checkpoint --blocksize=100m --iodepth=64 --readwrite=read

        # Clean up temporary directories for fio benchmarks
        rm -rf /{scratch,training,checkpoint}-data/fio-benchmarks-${TAG}

    outputs: [instructions]
