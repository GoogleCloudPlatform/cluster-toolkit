# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---

blueprint_name: a3u-slurm-ubuntu-gcs

vars:
  # The following are supplied through the deployment.yaml file.
  deployment_name: # supply deployment name
  project_id: # supply project ID
  region: # supply region
  zone: # supply zone
  a3u_cluster_size: # supply cluster size
  a3u_reservation_name: # supply reservation name
  hns_gcs_bucket: # Name of HNS enabled GCS bucket
  # End of variables defined by deployment.yaml. The remainder
  # of this blueprint need not be modified.

  # Image settings
  base_image:
    project: ubuntu-os-accelerator-images
    family: ubuntu-accelerator-2204-amd64-with-nvidia-570
  image_build_machine_type: n2-standard-16
  build_slurm_from_git_ref: 6.10.6

  # Cluster env settings
  # net0 and filestore ranges must not overlap
  net0_range: 192.168.0.0/19
  filestore_ip_range: 192.168.32.0/24
  net1_range: 192.168.64.0/18
  rdma_net_range: 192.168.128.0/18

  # Cluster Settings
  local_ssd_mountpoint: /mnt/localssd
  instance_image:
    project: $(vars.project_id)
    family: $(vars.deployment_name)-u22
  disk_size_gb: 200
  nccl_plugin_version: v1.1.0
  benchmark_dir: $(ghpc_stage("system_benchmarks"))

  # Here we define a set of startup script runners that are used to configure
  # the controller node
  controller_runners:
  - type: shell
    destination: stage_scripts.sh
    content: |
      #!/bin/bash
      SLURM_ROOT=/opt/apps/adm/slurm
      PARTITION_NAME=a3ultra
      mkdir -m 0755 -p "${SLURM_ROOT}/scripts"
      mkdir -p "${SLURM_ROOT}/partition-${PARTITION_NAME}-epilog_slurmd.d"
      ln -s "/slurm/scripts/tools/gpu-test" "${SLURM_ROOT}/partition-${PARTITION_NAME}-epilog_slurmd.d/gpu-test.epilog_slurmd"
  - type: data
    destination: /opt/apps/system_benchmarks/run-nccl-tests-via-ramble.sh
    source: $(vars.benchmark_dir)/run-nccl-tests-via-ramble.sh
  - type: data
    destination: /opt/apps/system_benchmarks/run-nemo-via-ramble.sh
    source: $(vars.benchmark_dir)/run-nemo-via-ramble.sh
  - type: data
    destination: /opt/apps/system_benchmarks/run-hpl-via-ramble.sh
    source: $(vars.benchmark_dir)/run-hpl-via-ramble.sh
  - type: data
    destination: /opt/apps/system_benchmarks/README.md
    source: $(vars.benchmark_dir)/README.md

  # Shared runners between login and controller:
  # Configure an enroot config path
  shared_runners:
  - type: data
    destination: /etc/enroot/enroot.conf
    content: |
      ENROOT_CONFIG_PATH     ${HOME}/.enroot

  # Here we define a set of startup script runners that are used to configure
  # the A3-Ultra nodes
  # Set up enroot, using the local ssds for runtime/cache/data/temp storage.
  a3u_runners:
  - type: data
    destination: /etc/enroot/enroot.conf
    content: |
      ENROOT_CONFIG_PATH     ${HOME}/.enroot
      ENROOT_RUNTIME_PATH    $(vars.local_ssd_mountpoint)/${UID}/enroot/runtime
      ENROOT_CACHE_PATH      $(vars.local_ssd_mountpoint)/${UID}/enroot/cache
      ENROOT_DATA_PATH       $(vars.local_ssd_mountpoint)/${UID}/enroot/data
      ENROOT_TEMP_PATH       $(vars.local_ssd_mountpoint)/${UID}/enroot

  # Install NCCL Network Plugin
  - type: ansible-local
    destination: nccl_plugin.yml
    content: |
      ---
      - name: Install NCCL plugin for A3 Ultra series
        hosts: all
        become: true
        tasks:
        - name: Add SystemD unit for NCCL plugin installation
          ansible.builtin.copy:
            dest: /etc/systemd/system/nccl-plugin@.service
            mode: 0o0644
            content: |
              [Unit]
              After=network-online.target
              Before=slurmd.service

              [Service]
              Type=oneshot
              ExecStartPre=/usr/bin/rm -rf /usr/local/gib
              ExecStartPre=/usr/bin/mkdir -p /usr/local/gib
              ExecStartPre=/snap/bin/gcloud auth configure-docker --quiet us-docker.pkg.dev
              ExecStart=/usr/bin/docker run --rm --name nccl-gib-installer --volume /usr/local/gib:/var/lib/gib \
                  us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib:%i install --install-nccl

              [Install]
              WantedBy=slurmd.service
          notify:
          - Reload SystemD
        handlers:
        - name: Reload SystemD
          ansible.builtin.systemd:
            daemon_reload: true
        post_tasks:
        - name: Enable NCCL plugin SystemD unit
          ansible.builtin.service:
            name: nccl-plugin@$(vars.nccl_plugin_version).service
            state: started
            enabled: true

  - type: ansible-local
    destination: enable_dcgm.yml
    content: |
      ---
      - name: Enable NVIDIA DCGM on GPU nodes
        hosts: all
        become: true
        vars:
          enable_ops_agent: true
          enable_nvidia_dcgm: true
          enable_nvidia_persistenced: true
        tasks:
        - name: Update Ops Agent configuration
          ansible.builtin.blockinfile:
            path: /etc/google-cloud-ops-agent/config.yaml
            insertafter: EOF
            block: |
              metrics:
                receivers:
                  dcgm:
                    type: dcgm
                service:
                  pipelines:
                    dcgm:
                      receivers:
                        - dcgm
          notify:
          - Restart Google Cloud Ops Agent
        handlers:
        - name: Restart Google Cloud Ops Agent
          ansible.builtin.service:
            name: google-cloud-ops-agent.service
            state: "{{ 'restarted' if enable_ops_agent else 'stopped' }}"
            enabled: "{{ enable_ops_agent }}"
        post_tasks:
        - name: Enable Google Cloud Ops Agent
          ansible.builtin.service:
            name: google-cloud-ops-agent.service
            state: "{{ 'started' if enable_ops_agent else 'stopped' }}"
            enabled: "{{ enable_ops_agent }}"
        - name: Enable NVIDIA DCGM
          ansible.builtin.service:
            name: nvidia-dcgm.service
            state: "{{ 'started' if enable_nvidia_dcgm else 'stopped' }}"
            enabled: "{{ enable_nvidia_dcgm }}"
        - name: Enable NVIDIA Persistence Daemon
          ansible.builtin.service:
            name: nvidia-persistenced.service
            state: "{{ 'started' if enable_nvidia_persistenced else 'stopped' }}"
            enabled: "{{ enable_nvidia_persistenced }}"

  # Configure Cloud Storage FUSE
  - type: ansible-local
    destination: gcsfuse.yml
    content: |
      ---
      - name: Create LSSD optimized gcsfuse mount
        hosts: all
        become: true
        tasks:
        - name: Create gcsfuse rwx configuration
          ansible.builtin.copy:
            dest: /etc/gcsfuse-lssd.yml
            owner: root
            group: root
            mode: 0o644
            content: |
              # GCSFuse configuration optimized for performance.
              # Aligned with GCSFuse best practices for checkpointing workloads.
              file-cache:
                max-size-mb: -1 # unlimited cache size
                cache-file-for-range-read: true
                enable-parallel-downloads: true
                download-chunk-size-mb: 50
                parallel-downloads-per-file: 16
              cache-dir: /mnt/localssd # utilizes local SSD for cache
              metadata-cache:
                ttl-secs: -1 # Cache metadata indefinitely
                stat-cache-max-size-mb: -1 # Cache all stat entries
                type-cache-max-size-mb: -1 # Cache all type entries
                negative-ttl-secs: 0 # Do not cache negative lookups
              implicit-dirs: true
              file-system:
                dir-mode: "777"
                file-mode: "777"
                fuse-options: "allow_other,read_ahead_kb=1024"
                rename-dir-limit: 20000  # Set to 20000 for hierarchical buckets
                temp-dir: /mnt/localssd # utilizes local SSD for temp files
              write:
                enable-streaming-writes: true
              foreground: true

        - name: Create gcsfuse read-only configuration for input data
          ansible.builtin.copy:
            dest: /etc/gcsfuse-ro.yml
            owner: root
            group: root
            mode: 0o644
            content: |
              # GCSFuse configuration optimized for read-only performance.
              # Aligned with GCSFuse best practices for training workloads.
              # File cache can be optionally enabled
              # file-cache:
              #   max-size-mb: -1
              #   cache-file-for-range-read: true
              #   enable-parallel-downloads: true
              cache-dir: /mnt/localssd
              metadata-cache:
                negative-ttl-secs: 0 # Do not cache negative lookups
                ttl-secs: -1  # Cache metadata indefinitely.
                stat-cache-max-size-mb: -1 # Cache all stat entries
                type-cache-max-size-mb: -1 # Cache all type entries
              implicit-dirs: true
              file-system:
                dir-mode: "755" # need 5 on dir to enable ls
                file-mode: "644"
                fuse-options: "allow_other,read_ahead_kb=1024"
              write:
                enable-streaming-writes: true
              foreground: true

        - name: Create gcsfuse systemd service
          ansible.builtin.copy:
            dest: /etc/systemd/system/gcsfuse-lssd.service
            owner: root
            group: root
            mode: 0o644
            content: |
              [Unit]
              Description=gcsfuse mount of all buckets
              After=local-fs.target

              [Service]
              Type=simple
              User=root
              ExecStartPre=/bin/mkdir -p /gcs
              ExecStart=gcsfuse --config-file /etc/gcsfuse-lssd.yml $(vars.hns_gcs_bucket) /gcs
              ExecStop=fusermount3 -u /gcs

              [Install]
              WantedBy=slurmd.service multi-user.target

        - name: Create read-only gcsfuse systemd service
          ansible.builtin.copy:
            dest: /etc/systemd/system/gcsfuse-ro.service
            owner: root
            group: root
            mode: 0o644
            content: |
              [Unit]
              Description=gcsfuse-ro mount
              After=local-fs.target

              [Service]
              Type=simple
              User=root
              ExecStartPre=/bin/mkdir -p /gcs-ro
              ExecStart=gcsfuse --config-file /etc/gcsfuse-ro.yml $(vars.hns_gcs_bucket) /gcs-ro
              ExecStop=fusermount3 -u /gcs-ro

              [Install]
              WantedBy=slurmd.service multi-user.target

        post_tasks:
        - name: Enable and restart gcsfuse
          ansible.builtin.service:
            name: gcsfuse-lssd.service
            state: restarted
            enabled: true

        - name: Enable and restart gcsfuse-ro
          ansible.builtin.service:
            name: gcsfuse-ro.service
            state: restarted
            enabled: true

  # Configure Cloud Storage FUSE for login/controller nodes
  gcsfuse_runners:
  - type: ansible-local
    destination: gcsfuse.yml
    content: |
      ---
      - name: Create Standard RWX gcsfuse mount
        hosts: localhost
        become: true
        tasks:
        - name: Create gcsfuse configuration
          ansible.builtin.copy:
            dest: /etc/gcsfuse.yml
            owner: root
            group: root
            mode: 0o644
            content: |
              # GCSFuse configuration for login/controller nodes.
              implicit-dirs: true
              file-system:
                dir-mode: "777"
                file-mode: "777"
                fuse-options: "allow_other,read_ahead_kb=1024"
              write:
                enable-streaming-writes: true
              foreground: true

        - name: Create gcsfuse systemd service
          ansible.builtin.copy:
            dest: /etc/systemd/system/gcsfuse.service
            owner: root
            group: root
            mode: 0o644
            content: |
              [Unit]
              Description=gcsfuse mount of all buckets
              After=local-fs.target

              [Service]
              Type=simple
              User=root
              ExecStartPre=/bin/mkdir -p /gcs
              ExecStart=gcsfuse --config-file /etc/gcsfuse.yml $(vars.hns_gcs_bucket) /gcs
              ExecStop=fusermount3 -u /gcs

              [Install]
              WantedBy=slurmd.service multi-user.target

        post_tasks:
        - name: Enable and restart gcsfuse
          ansible.builtin.service:
            name: gcsfuse.service
            state: restarted
            enabled: true

deployment_groups:
- group: image-env
  modules:
  - id: slurm-image-network
    source: modules/network/vpc

  - id: slurm-build-script
    source: modules/scripts/startup-script
    settings:
      install_ansible: true
      docker:
        enabled: true
      runners:
      - type: data
        destination: /etc/cluster_toolkit/a3ultra-prod-slurm-image.yaml
        source: ../.ghpc/artifacts/expanded_blueprint.yaml
      - type: data
        destination: /var/tmp/slurm_vars.json
        content: |
          {
            "reboot": false,
            "install_cuda": false,
            "install_gcsfuse": true,
            "install_lustre": false,
            "install_nvidia_repo": true,
            "install_ompi": true,
            "allow_kernel_upgrades": false,
            "monitoring_agent": "cloud-ops",
          }
      - type: shell
        destination: install_slurm.sh
        content: |
          #!/bin/bash
          set -e -o pipefail
          ansible-pull \
              -U https://github.com/GoogleCloudPlatform/slurm-gcp -C $(vars.build_slurm_from_git_ref) \
              -i localhost, --limit localhost --connection=local \
              -e @/var/tmp/slurm_vars.json \
              ansible/playbook.yml
            # this duplicates the ulimits configuration of the HPC VM Image
      - type: data
        destination: /etc/security/limits.d/99-unlimited.conf
        content: |
          * - memlock unlimited
          * - nproc unlimited
          * - stack unlimited
          * - nofile 1048576
          * - cpu unlimited
          * - rtprio unlimited
      - type: data
        destination: /etc/netplan/60-cloud-mrdma-init.yaml
        content: |
          network:
            ethernets:
              primary:
                match:
                  name: enp0s*
                  driver: gve
                dhcp4: true
                dhcp4-overrides:
                  use-domains: true
                dhcp6: true
                dhcp6-overrides:
                  use-domains: true
                optional: true
              secondary:
                match:
                  driver: gve
                dhcp4: true
                dhcp4-overrides:
                  use-domains: false
                  use-dns: false
                  use-ntp: false
                dhcp6: true
                dhcp6-overrides:
                  use-domains: false
                  use-dns: false
                  use-ntp: false
                optional: true
              mrdma_devices:
                match:
                  driver: mlx5_core
                dhcp-identifier: mac
                dhcp4: true
                dhcp4-overrides:
                  use-domains: true
                  use-dns: false
                  use-ntp: false
                optional: true
            version: 2
      - type: ansible-local
        destination: configure_gpu.yml
        content: |
          ---
          - name: Install NVIDIA packages
            hosts: all
            become: true
            vars:
              distribution: "{{ ansible_distribution | lower }}{{ ansible_distribution_version | replace('.','') }}"
              cuda_repo_url: https://developer.download.nvidia.com/compute/cuda/repos/{{ distribution }}/x86_64/cuda-keyring_1.1-1_all.deb
              cuda_repo_filename: /tmp/{{ cuda_repo_url | basename }}
              enable_nvidia_dcgm: false
              nvidia_packages:
              - cuda-toolkit-12-8
              - datacenter-gpu-manager
              - datacenter-gpu-manager-4-dev
              - libnvidia-cfg1-570-server
              - libnvidia-nscq-570
              - nvidia-compute-utils-570-server
            tasks:
            - name: Download NVIDIA repository package
              ansible.builtin.get_url:
                url: "{{ cuda_repo_url }}"
                dest: "{{ cuda_repo_filename }}"
            - name: Install NVIDIA repository package
              ansible.builtin.apt:
                deb: "{{ cuda_repo_filename }}"
                state: present
            - name: Reduce NVIDIA repository priority
              ansible.builtin.copy:
                dest: /etc/apt/preferences.d/cuda-repository-pin-600
                mode: 0o0644
                owner: root
                group: root
                content: |
                  Package: nsight-compute
                  Pin: origin *ubuntu.com*
                  Pin-Priority: -1

                  Package: nsight-systems
                  Pin: origin *ubuntu.com*
                  Pin-Priority: -1

                  Package: *
                  Pin: release l=NVIDIA CUDA
                  Pin-Priority: 400
            - name: Install NVIDIA fabric and CUDA
              ansible.builtin.apt:
                name: "{{ item }}"
                update_cache: true
              loop: "{{ nvidia_packages }}"
            - name: Freeze NVIDIA fabric and CUDA
              ansible.builtin.dpkg_selections:
                name: "{{ item }}"
                selection: hold
              loop: "{{ nvidia_packages }}"
            - name: Create nvidia-persistenced override directory
              ansible.builtin.file:
                path: /etc/systemd/system/nvidia-persistenced.service.d
                state: directory
                owner: root
                group: root
                mode: 0o755
            - name: Configure nvidia-persistenced override
              ansible.builtin.copy:
                dest: /etc/systemd/system/nvidia-persistenced.service.d/persistence_mode.conf
                owner: root
                group: root
                mode: 0o644
                content: |
                  [Service]
                  ExecStart=
                  ExecStart=/usr/bin/nvidia-persistenced --user nvidia-persistenced --verbose
              notify: Reload SystemD
            handlers:
            - name: Reload SystemD
              ansible.builtin.systemd:
                daemon_reload: true
            post_tasks:
            - name: Disable NVIDIA DCGM by default (enable during boot on GPU nodes)
              ansible.builtin.service:
                name: nvidia-dcgm.service
                state: stopped
                enabled: false
            - name: Disable nvidia-persistenced SystemD unit (enable during boot on GPU nodes)
              ansible.builtin.service:
                name: nvidia-persistenced.service
                state: stopped
                enabled: false
      - type: ansible-local
        destination: install_mellanox_drivers.yml
        content: |
          ---
          - name: Update Netplan and Install Network Utils
            hosts: all
            become: true
            tasks:
            - name: Install Linux Modules Extra
              ansible.builtin.package:
                name:
                - ibverbs-utils
                state: present
            - name: Apply netplan
              ansible.builtin.command: netplan apply

- group: image
  modules:
  - id: slurm-a3ultra-image
    source: modules/packer/custom-image
    kind: packer
    settings:
      disk_size: $(vars.disk_size_gb)
      machine_type: $(vars.image_build_machine_type)
      source_image_family: $(vars.base_image.family)
      source_image_project_id: [$(vars.base_image.project)]
      image_family: $(vars.instance_image.family)
      omit_external_ip: false
    use:
    - slurm-image-network
    - slurm-build-script

- group: cluster-env
  modules:
  - id: a3ultra-slurm-net-0
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-0
      mtu: 8896
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-0
        subnet_region: $(vars.region)
        subnet_ip: $(vars.net0_range)

  - id: a3ultra-slurm-net-1
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-1
      mtu: 8896
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-1
        subnet_region: $(vars.region)
        subnet_ip: $(vars.net1_range)

  - id: a3ultra-slurm-rdma-net
    source: modules/network/gpu-rdma-vpc
    settings:
      network_name: $(vars.deployment_name)-rdma-net
      network_profile: https://www.googleapis.com/compute/beta/projects/$(vars.project_id)/global/networkProfiles/$(vars.zone)-vpc-roce
      network_routing_mode: REGIONAL
      subnetworks_template:
        name_prefix: $(vars.deployment_name)-mrdma-sub
        count: 8
        ip_range: $(vars.rdma_net_range)
        region: $(vars.region)

  - id: homefs
    source: modules/file-system/filestore
    use:
    - a3ultra-slurm-net-0
    settings:
      filestore_tier: HIGH_SCALE_SSD
      size_gb: 10240
      local_mount: /home
      reserved_ip_range: $(vars.filestore_ip_range)
      deletion_protection:
        enabled: true
        reason: Avoid data loss
    outputs:
    - network_storage

- group: cluster
  modules:
  - id: a3ultra_startup
    source: modules/scripts/startup-script
    settings:
      local_ssd_filesystem:
        mountpoint: $(vars.local_ssd_mountpoint)
        permissions: "1777" # must quote numeric filesystem permissions!
      docker:
        enabled: true
        world_writable: true
        daemon_config: |
          {
            "data-root": "$(vars.local_ssd_mountpoint)/docker"
          }
      runners: $(flatten([vars.a3u_runners]))

  - id: a3_ultra_nodeset
    source: community/modules/compute/schedmd-slurm-gcp-v6-nodeset
    use: [a3ultra-slurm-net-0, a3ultra_startup]
    settings:
      bandwidth_tier: gvnic_enabled
      machine_type: a3-ultragpu-8g
      enable_public_ips: true
      node_count_static: $(vars.a3u_cluster_size)
      node_count_dynamic_max: 0
      enable_placement: false
      disk_type: hyperdisk-balanced
      on_host_maintenance: TERMINATE
      reservation_name: $(vars.a3u_reservation_name)
      advanced_machine_features:
        threads_per_core: null # Use platform default value
      node_conf:
        CoresPerSocket: 56
        ThreadsPerCore: 2
      additional_networks:
        $(concat(
          [{
            network=null,
            subnetwork=a3ultra-slurm-net-1.subnetwork_self_link,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip="",
            stack_type=null,
            access_config=[],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
          a3ultra-slurm-rdma-net.subnetwork_interfaces
        ))

  - id: a3_ultra_partition
    source: community/modules/compute/schedmd-slurm-gcp-v6-partition
    use:
    - a3_ultra_nodeset
    settings:
      exclusive: false
      partition_name: a3ultra
      is_default: true
      partition_conf:
        ResumeTimeout: 1200
        SuspendTimeout: 600
        OverSubscribe: EXCLUSIVE

  - id: controller_startup
    source: modules/scripts/startup-script
    settings:
      runners: $(flatten([vars.shared_runners, vars.controller_runners, vars.gcsfuse_runners]))

  - id: login_startup
    source: modules/scripts/startup-script
    settings:
      runners: $(flatten([vars.shared_runners, vars.gcsfuse_runners]))

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-login
    use: [a3ultra-slurm-net-0]
    settings:
      disk_size_gb: 300
      enable_login_public_ips: true
      machine_type: n2-standard-8

  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-controller
    use:
    - a3ultra-slurm-net-0
    - a3_ultra_partition
    - slurm_login
    - homefs
    settings:
      enable_controller_public_ips: true
      disk_type: pd-extreme
      disk_size_gb: 300
      machine_type: n2-standard-80
      controller_startup_script: $(controller_startup.startup_script)
      login_startup_script: $(login_startup.startup_script)
      enable_external_prolog_epilog: true
