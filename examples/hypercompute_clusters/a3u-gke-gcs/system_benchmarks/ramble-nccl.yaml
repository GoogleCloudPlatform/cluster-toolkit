# Copyright 2025 "Google LLC"
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
apiVersion: v1
kind: Namespace
metadata:
  name: ramble
  namespace: ramble
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: "ramble"
  name: "a3-ultra-tas"
spec:
  clusterQueue: "a3-ultra"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ramble
  namespace: ramble
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: ramble
  name: ramble-editor
rules:
- apiGroups: ["", "batch", "jobset.x-k8s.io", "kueue.x-k8s.io"] # "" indicates the core API group
  resources: ["*"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kueue-reader
rules:
- apiGroups: ["kueue.x-k8s.io"]
  resources: ["clusterqueues"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ramble-editor
  namespace: ramble
subjects:
- kind: ServiceAccount
  name: ramble
  apiGroup: ""
roleRef:
  kind: Role
  name: ramble-editor
  apiGroup: ""
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ramble-kueue-reader
subjects:
- kind: ServiceAccount
  name: ramble
  namespace: ramble
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: kueue-reader
  apiGroup: ""
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ramble-nccl-configs
  namespace: ramble
data:
  execute_nccl.tpl: |
    #!/bin/bash
    set -e
    cd "{experiment_run_dir}"
    printf "Submitting {experiment_name}\n"
    kubectl create -f jobset 2>&1 | tee klog

  collect_logs.tpl: |
    #!/bin/bash
    set -e
    jobname=$(head -n 1 {experiment_run_dir}/klog | awk -F " |/" '{print $2}')
    printf "Waiting for up to a day for ${jobname} to complete.\n"
    kubectl wait --timeout=86400s jobs/${jobname}-w-0 --for=condition=complete
    kubectl logs --tail=-1 -f -l batch.kubernetes.io/job-completion-index=0,job-name=${jobname}-w-0 | tee {log_file}

  ramble.yaml: |
    ramble:
      variables:
        ssh_port: 222
        batch_submit: '{execute_nccl}'
        mpi_command: >-
          mpirun
          -n {n_ranks}
          -N {processes_per_node}
          --bind-to none
          --hostfile /tmp/hostfile
          --mca btl self,tcp
          --mca btl_tcp_if_include eth0
          --mca orte_keep_fqdn_hostnames 1
          --mca plm_rsh_no_tree_spawn 1
          -x {mpi_env_vars}
          --mca plm_rsh_agent "ssh -q -o LogLevel=ERROR -o StrictHostKeyChecking=no -p {ssh_port}"
        mpi_env_vars: >-
          $(echo
          ${!NCCL*}
          ${!OMPI*}
          LD_LIBRARY_PATH
          | sed 's/ / -x /g')

        container_name: nccl-tests
        container_uri: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-diagnostic:v1.0.5
        gke_container_name: nccl
        gke_namespace: ramble
        gpus_per_node: 8
        nccl-tests_path: /third_party/nccl-tests-mpi
        processes_per_node: 8

        # Potentially need to be modified
        gke_nodepool: a3-ultragpu-8g-a3-ultragpu-pool
        sysnet_subnet_prefix: a3u-gke-gcs-sub
        gpu_subnet_prefix: a3u-gke-gcs-rdma-sub
        cluster_queue: a3-ultra-tas

      env_vars:
        set:
          CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
          NCCL_NET: gIB
        prepend:
        - paths:
            LD_LIBRARY_PATH: /usr/local/gib/lib64

      applications:
        nccl-tests:
          workloads:
            '{workload}':
              experiments:
                '{workload}-{n_nodes}':
                  variables:
                    n_nodes: [2,4,8,16,32]

                    jobset_name: ['ag-{n_nodes}', 'ar-{n_nodes}', 'rs-{n_nodes}']
                    workload: [all-gather, all-reduce, reduce-scatter]
                    binary: [all_gather_perf, all_reduce_perf, reduce_scatter_perf]
                  zips:
                    bench:
                    - jobset_name
                    - workload
                    - binary
                  matrix:
                  - bench
                  - n_nodes

          internals:
            custom_executables:
              mpi_head_node:
                template:
                - cd /third_party/nccl-tests/build/
                - source /usr/local/gib/scripts/set_nccl_env.sh
                - if [[ "${NODE_RANK}" -eq "0" ]]; then
                redirect: ''
                log_file: ''
              wait_worker_nodes:
                template:
                - else
                - while ping -c 1 ${WORKERS_BASENAME}-0.${POSTFIX}; do
                - sleep 5
                - done
                - fi
                redirect: ''
                log_file: ''
              tail_log:
                template:
                - tail -f {log_file} &
                - export TAIL_PID=$!
                redirect: ''
                log_file: ''
              kill_tail:
                template:
                - kill -9 $TAIL_PID
                redirect: ''
                log_file: ''
            executable_injection:
            - name: mpi_head_node
              order: before
            - name: wait_worker_nodes
              order: after
            - name: tail_log
              order: before
            - name: kill_tail
              order: after
          formatted_executables:
            yaml_command:
              indentation: 18
              join_separator: \n
              commands:
              - mkdir -p {experiment_run_dir}
              - ulimit -l unlimited
              - '{unformatted_command}'

  jobset.tpl: |
    apiVersion: jobset.x-k8s.io/v1alpha2
    kind: JobSet
    metadata:
      generateName: {jobset_name}-
      namespace: {gke_namespace}
      labels:
        kueue.x-k8s.io/queue-name: {cluster_queue}
    spec:
      ttlSecondsAfterFinished: 86400
      network:
        enableDNSHostnames: true
        publishNotReadyAddresses: true
      replicatedJobs:
        - name: w
          template:
            spec:
              parallelism: {n_nodes}
              completions: {n_nodes}
              template:
                metadata:
                  annotations:
                    kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
                    networking.gke.io/default-interface: 'eth0'
                    networking.gke.io/interfaces: |
                      [
                        \{"interfaceName":"eth0","network":"default"\},
                        \{"interfaceName":"eth1","network":"{sysnet_subnet_prefix}-1"\},
                        \{"interfaceName":"eth2","network":"{gpu_subnet_prefix}-0"\},
                        \{"interfaceName":"eth3","network":"{gpu_subnet_prefix}-1"\},
                        \{"interfaceName":"eth4","network":"{gpu_subnet_prefix}-2"\},
                        \{"interfaceName":"eth5","network":"{gpu_subnet_prefix}-3"\},
                        \{"interfaceName":"eth6","network":"{gpu_subnet_prefix}-4"\},
                        \{"interfaceName":"eth7","network":"{gpu_subnet_prefix}-5"\},
                        \{"interfaceName":"eth8","network":"{gpu_subnet_prefix}-6"\},
                        \{"interfaceName":"eth9","network":"{gpu_subnet_prefix}-7"\}
                      ]
                spec:
                  restartPolicy: Never
                  nodeSelector:
                    cloud.google.com/gke-nodepool: {gke_nodepool}
                    cloud.google.com/gke-ephemeral-storage-local-ssd: "true"
                  tolerations:
                  - key: cloud.google.com/gke-queued
                    effect: NoSchedule
                    value: "true"
                  - key: "nvidia.com/gpu"
                    operator: "Exists"
                    effect: "NoSchedule"
                  setHostnameAsFQDN: true
                  volumes:
                  - name: mpi-id
                    secret:
                      secretName: mpi-ssh-nccl
                      items:
                      - key: ssh-privatekey
                        path: "id_rsa"
                      - key: ssh-publickey
                        path: "id_rsa.pub"
                  - name: gib
                    hostPath:
                      path: /home/kubernetes/bin/gib
                  - name: nvidia
                    hostPath:
                      path: /home/kubernetes/bin/nvidia
                  - name: lib64
                    hostPath:
                      path: /lib64
                  - name: shared-memory
                    emptyDir:
                      medium: "Memory"
                      sizeLimit: 250Gi
                  - name: sys
                    hostPath:
                      path: /sys
                  - name: proc-sys
                    hostPath:
                      path: /proc/sys
                  - name: local-ssd
                    hostPath:
                      path: /mnt/stateful_partition/kube-ephemeral-ssd
                  initContainers:
                  - name: gpu-healthcheck
                    image: alpine:latest
                    command: ["/bin/sh", "-c"]
                    args:
                    - |
                      apk add --no-cache bash  # Install bash
                      /bin/bash -c "set -ex
                      NUM_GPUS=$(/usr/local/nvidia/bin/nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits | wc -l)
                      if [ \${NUM_GPUS} -lt 8 ]; then
                        echo \"Error: Only \${NUM_GPUS} GPUs and expected 8\"
                        exit 1
                      fi
                      gpu_errors=(\$(/usr/local/nvidia/bin/nvidia-smi --query-gpu=ecc.errors.uncorrected.volatile.total --format=csv,noheader,nounits))
                      for gpu_index in \${!gpu_errors[@]}; do
                          if [ \${gpu_errors[\$gpu_index]} == '[N/A]' ]; then
                              echo 'Error: ERR detected in GPU index '\$gpu_index
                              exit 1
                          elif [ \${gpu_errors[\$gpu_index]} -gt 0 ]; then
                              echo 'Error: Unrecoverable ECC errors detected in GPU index '\$gpu_index
                              exit 1
                          fi
                      done
                      echo \${NUM_GPUS} GPUs found with no ERR or Unrecoverable ECC errors"
                    volumeMounts:
                    - name: nvidia
                      mountPath: /usr/local/nvidia
                    - name: lib64
                      mountPath: /lib64
                    securityContext:
                      privileged: true
                    env:
                    - name: LD_LIBRARY_PATH
                      value: /usr/local/nvidia/lib64
                  containers:
                  - name: {gke_container_name}
                    stdin: true
                    tty: true
                    image: {container_uri}
                    env:
                    - name: OMPI_ALLOW_RUN_AS_ROOT
                      value: "1"
                    - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
                      value: "1"
                    - name: MY_NODE_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.nodeName
                    command:
                    - bash
                    - -c
                    - |
                      set -x

                      # Setup SSH
                      export DEBIAN_FRONTEND=noninteractive

                      apt update -qq -y
                      apt install -qq -y iputils-ping openssh-server

                      mkdir -p /run/sshd ~/.ssh
                      chmod 700 ~/.ssh
                      cp /secrets/ssh/* ~/.ssh/
                      cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
                      chmod 600 ~/.ssh/*
                      mkdir -p /run/sshd
                      /sbin/sshd

                      # Load all the cuda libs
                      /sbin/ldconfig

                      export POSTFIX=$(hostname | cut -d . -f 2-)
                      export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )
                      export NODE_RANK=$JOB_COMPLETION_INDEX

                      # For every host, get the entity and add to hostfile
                      for i in `seq 0 $(({n_nodes}-1))`; do
                        OTHER=${WORKERS_BASENAME}-${i}.${POSTFIX}
                        until ssh -p {ssh_port} -o StrictHostKeyChecking=no $OTHER hostname;
                        do
                          echo ...
                          sleep 10
                        done
                        echo ${OTHER} port={ssh_port} slots={processes_per_node} | tee -a /tmp/hostfile;
                      done
                      cat /tmp/hostfile

    {yaml_command}

                      exit 0

                    volumeMounts:
                    - name: mpi-id
                      mountPath: "/secrets/ssh"
                      readOnly: true
                    - name: nvidia
                      mountPath: /usr/local/nvidia
                    - name: gib
                      mountPath: /usr/local/gib
                    - name: shared-memory
                      mountPath: /dev/shm
                    - name: local-ssd
                      mountPath: /ssd
                    resources:
                      limits:
                        nvidia.com/gpu: 8
                      requests:
                        nvidia.com/gpu: 8

                  restartPolicy: Never

---
apiVersion: batch/v1
kind: Job
metadata:
  name: ramble-nccl-runner
  namespace: ramble
spec:
  template:
    spec:
      volumes:
      - name: config
        configMap:
          name: ramble-nccl-configs
          items:
          - key: jobset.tpl
            path: jobset.tpl
          - key: ramble.yaml
            path: ramble.yaml
          - key: execute_nccl.tpl
            path: execute_nccl.tpl
          - key: collect_logs.tpl
            path: collect_logs.tpl

      serviceAccountName: ramble
      containers:
      - name: ramble-controller
        image: ubuntu:latest

        volumeMounts:
        - name: config
          mountPath: /opt/configs/
          readOnly: true

        command:
        - bash
        - -c
        - |
          export DEBIAN_FRONTEND=noninteractive

          set -e
          printf "Installing system dependencies\n"
          apt update -qq -y > /dev/null
          apt install -qq -y build-essential python3-venv jq git curl > /dev/null

          printf "Installing kubectl\n"
          curl -s -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

          # Use current unix timestamp as a unique tag
          # for jobs submitted
          TAG=$(date +%s)
          TEST_DIR=/workspace/nccl-tests-"${TAG}"
          SOFTWARE_INSTALL=/opt

          mkdir -p ${SOFTWARE_INSTALL} ${TEST_DIR}

          printf "Cloning ramble and cluster-toolkit\n"
          git clone --depth 1 -c feature.manyFiles=true https://github.com/GoogleCloudPlatform/ramble.git "${SOFTWARE_INSTALL}"/ramble

          printf "Setting up ramble python environment, and installing requirements\n"
          python3 -m venv "${SOFTWARE_INSTALL}"/ramble/env || true
          source "${SOFTWARE_INSTALL}"/ramble/env/bin/activate
          pip install -q -r "${SOFTWARE_INSTALL}"/ramble/requirements.txt

          # Activate ramble
          . ${SOFTWARE_INSTALL}/ramble/share/ramble/setup-env.sh

          ramble workspace create -a -d "${TEST_DIR}"

          cp /opt/configs/* ${RAMBLE_WORKSPACE}/configs/

          cd ${RAMBLE_WORKSPACE}

          # Set up SSH
          printf "Creating ssh keypair for MPI workloads\n"
          ssh-keygen -b 2048 -f mpi_id -N ""
          kubectl create secret generic mpi-ssh-nccl --from-file=ssh-privatekey=./mpi_id --from-file=ssh-publickey=./mpi_id.pub || true

          # Get number of GPUs / nodes available in this cluster from Kueue:
          AVAILABLE_GPUS=$(
            kubectl get clusterqueues.kueue.x-k8s.io -o json |
            jq -r '.items[].spec.resourceGroups[].flavors[] | select (.name=="a3-ultra-tas") |
            .resources[] | select (.name="nvidia.com/gpu") | .nominalQuota'
          )

          N_NODES=$((AVAILABLE_GPUS / 8))

          printf "\n--- Available Benchmarks on %s nodes --\n" ${N_NODES}
          ramble workspace info --where '{n_nodes} <= '"${N_NODES}"

          printf "\n--------- Setting up Benchmarks -------\n"
          ramble workspace setup --where '{n_nodes} <= '"${N_NODES}"

          printf "\n----------- Running Benchmarks --------\n"
          ramble on --where '{n_nodes} <= '"${N_NODES}"

          printf "\n------- Collecting benchmark logs -----\n"
          ramble on --executor "{experiment_run_dir}/collect_logs" --where '{n_nodes} <= '"${N_NODES}"

          printf "\n------- Analyzing benchmark logs ------\n"
          ramble workspace analyze -f json --where '{n_nodes} <= '"${N_NODES}"

          printf "\n------- Archiving ramble workspace ------\n"
          ramble workspace archive -t --where '{n_nodes} <= '"${N_NODES}"

          printf "\n---- SUMMARY for >1GB Message Sizes ----\n"
          jq -r '["workload","n_nodes","msg_size","busbw"], (.experiments[] as $exp | $exp.CONTEXTS[] as $context |
          {
            experiment_name: $exp.name,
            workload: $exp.workload_name,
            n_nodes: $exp.n_nodes,
            Context: $context.name
          } +
          ($context.foms | from_entries )
          | select(.Size | tonumber > 1000000000)
          | [.workload, .n_nodes, .Size, ."Out of Place Bus Bandwidth"])
          | @tsv' results.latest.json
          printf "\n-------- Benchmarking Complete -------\n"

          ARCHIVE_TAR=$(readlink archive/archive.latest.tar.gz)
          ARCHIVE_PATH=${RAMBLE_WORKSPACE}/archive/${ARCHIVE_TAR}
          RESULTS_FILE=$(basename $(readlink results.latest.json))
          RESULTS_PATH=${RAMBLE_WORKSPACE}/${RESULTS_FILE}

          printf "\n# To copy the full results from container:\n"
          printf "kubectl cp %s:%s %s\n" $(hostname) ${RESULTS_PATH} ${RESULTS_FILE}
          printf "\n# To copy the ramble workspace archive from container:\n"
          printf "kubectl cp %s:%s ./%s\n" $(hostname) ${ARCHIVE_PATH} ${ARCHIVE_TAR}

          printf "\n# To re-activate ramble workspace, first access runner:\n"
          printf "kubectl exec -it %s -- /bin/bash\n" $(hostname)
          printf "# Then run:\n"
          printf "cd ${RAMBLE_WORKSPACE}\n"
          printf "source "${SOFTWARE_INSTALL}"/ramble/env/bin/activate\n"
          printf ". ${SOFTWARE_INSTALL}/ramble/share/ramble/setup-env.sh\n"
          printf "ramble workspace activate .\n"

          printf "\n- Sleeping for 1 day to allow introspection -\n"
          sleep 86400


      restartPolicy: Never
  backoffLimit: 4
