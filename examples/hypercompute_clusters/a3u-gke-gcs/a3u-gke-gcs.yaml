# Copyright 2024 "Google LLC"
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: a3u-gke-gcs

vars:
  project_id: # Insert GCP project
  deployment_name: # Unique name of this cluster, like a3u-gke-gcs
  region: # Region, e.g. europe-west1
  zone: # Zone, e.g. europe-west1-b

  # Cidr block containing the IP of the machine calling terraform and kubectl
  # The value can be more specific if the IPs are known which will run kubectl
  # e.g. the local system running Terraform or a remote node
  authorized_cidr: 0.0.0.0/0
  extended_reservation:  # Reservation name, e.g. <project>/<reservation-name>/reservationBlocks/<reservation-block-name>

  nccl_installer_path: $(ghpc_stage("./nccl-rdma-installer.yaml"))
  mtu_size: 8896
  static_node_count: # Number of A3-Ultra nodes, e.g. 2
  # Number of H200 GPUs (for later use by Kueue), which
  # should be 8 x `static_node_count`
  num_gpus:
  training_bucket_name: # Name of bucket that holds training data
  checkpoint_bucket_name: # Name of bucket used for checkpoints
  system_node_pool_disk_size_gb: 200
  a3ultra_node_pool_disk_size_gb: 100

deployment_groups:
- group: primary
  modules:
  - id: gke-a3-ultra-net-0
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-0
      mtu: 8896
      subnetworks:
      - subnet_name: $(vars.deployment_name)-sub-0
        subnet_region: $(vars.region)
        subnet_ip: 192.168.0.0/18
      secondary_ranges_list:
      - subnetwork_name: $(vars.deployment_name)-sub-0
        ranges:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: gke-a3-ultra-net-1
    source: modules/network/vpc
    settings:
      network_name: $(vars.deployment_name)-net-1
      mtu: $(vars.mtu_size)
      subnetworks:
      - subnet_name: gke-a3u-gcs-sub-1
        subnet_region: $(vars.region)
        subnet_ip: 192.168.64.0/18

  - id: gke-a3-ultra-rdma-net
    source: github.com/GoogleCloudPlatform/cluster-toolkit.git//community/modules/network/rdma-vpc?ref=98c49fe
    settings:
      network_name: $(vars.deployment_name)-rdma-net
      mtu: $(vars.mtu_size)
      network_profile: https://www.googleapis.com/compute/beta/projects/$(vars.project_id)/global/networkProfiles/$(vars.zone)-vpc-roce
      network_routing_mode: REGIONAL
      subnetworks_template:
        name_prefix: $(vars.deployment_name)-rdma-sub
        count: 8
        ip_range: 192.168.128.0/18
        region: $(vars.region)

  - id: a3-ultragpu-cluster
    source: github.com/GoogleCloudPlatform/cluster-toolkit.git//modules/scheduler/gke-cluster?ref=e0c690b
    use: [gke-a3-ultra-net-0]
    settings:
      release_channel: RAPID
      system_node_pool_machine_type: "e2-standard-16"
      system_node_pool_disk_size_gb: $(vars.system_node_pool_disk_size_gb)
      system_node_pool_taints: []
      enable_dcgm_monitoring: true
      enable_gcsfuse_csi: true
      enable_private_endpoint: false # Allows access from authorized public IPs
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr) # Allows your machine to run the kubectl command. Required for multi network setup.
        display_name: "kubectl-access-network"
      maintenance_exclusions:
      - name: no-minor-or-node-upgrades-indefinite
        start_time: "2024-12-01T00:00:00Z"
        end_time: "2025-12-22T00:00:00Z"
        exclusion_scope: NO_MINOR_OR_NODE_UPGRADES
      additional_networks:
        $(concat(
          [{
            network=gke-a3-ultra-net-1.network_name,
            subnetwork=gke-a3-ultra-net-1.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
         gke-a3-ultra-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: a3-ultragpu-pool
    source: github.com/GoogleCloudPlatform/cluster-toolkit.git//modules/compute/gke-node-pool?ref=e0c690b
    use: [a3-ultragpu-cluster]
    settings:
      machine_type: a3-ultragpu-8g
      auto_upgrade: true
      zones: [$(vars.zone)]
      disk_type: hyperdisk-balanced
      disk_size_gb: $(vars.a3ultra_node_pool_disk_size_gb)
      static_node_count: $(vars.static_node_count)
      guest_accelerator:
      - type: nvidia-h200-141gb
        count: 8
        gpu_driver_installation_config:
          gpu_driver_version: "LATEST"
      reservation_affinity:
        consume_reservation_type: SPECIFIC_RESERVATION
        specific_reservations:
        - name: $(vars.extended_reservation)
      additional_networks:
        $(concat(
          [{
            network=gke-a3-ultra-net-1.network_name,
            subnetwork=gke-a3-ultra-net-1.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
         gke-a3-ultra-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: topology-aware-scheduler-install
    source: github.com/GoogleCloudPlatform/cluster-toolkit.git//community/modules/compute/gke-topology-scheduler?ref=e0c690b
    use: [a3-ultragpu-cluster]

  # Install Kueue, Jobset, and NCCL installer
  - id: workload-manager-install
    source: github.com/GoogleCloudPlatform/cluster-toolkit.git//modules/management/kubectl-apply?ref=e0c690b
    use: [a3-ultragpu-cluster]
    settings:
      kueue:
        install: true
        version: v0.9.1
        config_path: $(ghpc_stage("kueue-configuration.yaml.tftpl"))
        config_template_vars:
          num_gpus: $(vars.num_gpus)
      jobset:
        install: true
        version: v0.7.1
      apply_manifests:
      - source: $(vars.nccl_installer_path)

  # Create a remote mount of $(vars.training_bucket_name)
  # using mount options optimized for reading training
  # data.
  - id: gcs-training
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(vars.training_bucket_name)
      local_mount: /training-data
      fs_type: gcsfuse
      mount_options: "implicit-dirs, metadata-cache:ttl-secs:-1, metadata-cache:stat-cache-max-size-mb:-1, metadata-cache:type-cache-max-size-mb:-1, file-cache:max-size-mb:-1, file-cache:cache-file-for-range-read:true, file-system:kernel-list-cache-ttl-secs:-1"

  # Create a remote mount of $(vars.checkpoint_bucket_name)
  # using mount options optimized for writing and reading
  # checkpoint data.
  - id: gcs-checkpointing
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: $(vars.checkpoint_bucket_name)
      local_mount: /checkpoint-data
      fs_type: gcsfuse
      mount_options: "implicit-dirs, metadata-cache:ttl-secs:0, file-cache:max-size-mb:-1, file-cache:cache-file-for-range-read:true, file-system:kernel-list-cache-ttl-secs:0, file-cache:enable-parallel-downloads:true, rename-dir-limit=200000"

  # Persistent Volume for training data
  - id: training-pv
    source: modules/file-system/gke-persistent-volume
    use: [gcs-training, a3-ultragpu-cluster]
    settings:
      gcs_bucket_name: $(vars.training_bucket_name)
      capacity_gb: 1000000

  # Persistent Volume for checkpoint data
  - id: checkpointing-pv
    source: modules/file-system/gke-persistent-volume
    use: [gcs-checkpointing, a3-ultragpu-cluster]
    settings:
      gcs_bucket_name: $(vars.checkpoint_bucket_name)
      capacity_gb: 1000000

  # This is an example job that will install and run an `fio`
  # benchmark against the training and checkpointing buckets.
  - id: fio-bench-job-template
    source: modules/compute/gke-job-template
    use: [checkpointing-pv, training-pv, a3-ultragpu-pool]
    settings:
      ephemeral_volumes:
      - type: local-ssd
        mount_path: /scratch-data
        size_gb: 1000  # Use 1 out of 12 TB for local scratch

      k8s_service_account_name: default
      image: ubuntu:latest

      command:
      - bash
      - -c
      - |

        set -eux
        export DEBIAN_FRONTEND=noninteractive

        # Install fio
        apt update -y && apt install -y fio

        # Use a tag to create a unique path for tests
        TAG=`date +%s`

        # Verify mountpoints
        df -h
        mountpoint /scratch-data
        mountpoint /checkpoint-data
        mountpoint /training-data

        # Create temporary directory for fio benchmarks
        mkdir -p /{scratch,training,checkpoint}-data/fio-benchmarks-${TAG}

        # The following will take roughly 10 minutes to complete

        # Perform scratch data write performance test
        fio --ioengine=libaio --filesize=10G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/scratch-data/fio-benchmarks-${TAG} \
          --name=scratch --blocksize=100m --iodepth=64 --readwrite=write

        # Perform training data reading performance test
        fio --ioengine=libaio --filesize=1G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/training-data/fio-benchmarks-${TAG} \
          --name=training --blocksize=1m --iodepth=64 --readwrite=randread

        # Perform checkpoint data writing performance test
        fio --ioengine=libaio --filesize=10G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/checkpoint-data/fio-benchmarks-${TAG} \
          --name=checkpoint --blocksize=100m --iodepth=64 --readwrite=write

        # Perform checkpoint data reading performance test
        fio --ioengine=libaio --filesize=10G --ramp_time=2s --runtime=1m \
          --numjobs=32 --create_serialize=0 --direct=1 --verify=0 \
          --randrepeat=0 --group_reporting --directory=/checkpoint-data/fio-benchmarks-${TAG} \
          --name=checkpoint --blocksize=100m --iodepth=64 --readwrite=read

        # Clean up temporary directories for fio benchmarks
        rm -rf /{scratch-training,checkpoint}-data/fio-benchmarks-${TAG}

    outputs: [instructions]
