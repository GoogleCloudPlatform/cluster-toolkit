# Copyright 2024 "Google LLC"
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: a3ultra-staging-gke

vars:
  project_id: hpc-toolkit-dev-staging
  deployment_name: a3ultra-staging-gke
  region: us-east5
  zone: us-east5-staginga
  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.

  # For staging purposes authorized_cidr has been left completely open
  # The value can be more specific if the IPs are known which will run kubectl
  # e.g. the local system running Terraform or a remote node
  authorized_cidr: 0.0.0.0/0

validators:
- validator: test_zone_exists
  inputs: {}
  skip: true
- validator: test_zone_in_region
  inputs: {}
  skip: true
- validator: test_region_exists
  inputs: {}
  skip: true

terraform_providers:
  google:
    source: hashicorp/google
    version: 5.38.0
    configuration:
      project: $(vars.project_id)
      region: $(vars.region)
      zone: $(vars.zone)
      container_custom_endpoint: "https://hpc-toolkit-rdma-sandbox-1-test-container.sandbox.googleapis.com/"
      compute_custom_endpoint: "https://www.googleapis.com/compute/staging_v1/"

  google-beta:
    source: hashicorp/google-beta
    version: 5.38.0
    configuration:
      project: $(vars.project_id)
      region: $(vars.region)
      zone: $(vars.zone)
      container_custom_endpoint: "https://hpc-toolkit-rdma-sandbox-1-test-container.sandbox.googleapis.com/"
      compute_custom_endpoint: "https://www.googleapis.com/compute/staging_v1/"

  google-private:
    source: hashicorp/google-private
    version: 0.0.1962  # This version should not change - google-private is inherently brittle
    configuration:
      project: $(vars.project_id)
      region: $(vars.region)
      zone: $(vars.zone)
      container_custom_endpoint: "https://hpc-toolkit-rdma-sandbox-1-test-container.sandbox.googleapis.com/"
      compute_custom_endpoint: "https://www.googleapis.com/compute/staging_alpha/"

deployment_groups:
- group: primary
  modules:
  - id: a3ultra-gke-net-0
    source: modules/network/vpc
    settings:
      network_name: a3ultra-gke-net-0
      allowed_ssh_ip_ranges: [0.0.0.0/0]
      subnetworks:
      - subnet_name: a3ultra-gke-sub-0
        subnet_region: $(vars.region)
        subnet_ip: 192.168.0.0/18
      secondary_ranges:
        a3ultra-gke-sub-0:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20
      firewall_rules:
      - name: a3ultra-gke-internal-0
        ranges: [192.168.0.0/16]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: a3ultra-gke-net-1
    source: modules/network/vpc
    settings:
      network_name: a3ultra-gke-net-1
      subnetworks:
      - subnet_name: a3ultra-gke-sub-1
        subnet_region: $(vars.region)
        subnet_ip: 192.168.64.0/18
      firewall_rules:
      - name: a3ultra-gke-internal-1
        ranges: [192.168.0.0/16]
        allow:
        - protocol: tcp
          ports: ["0-65535"]
        - protocol: udp
          ports: ["0-65535"]
        - protocol: icmp

  - id: a3ultra-gke-rdma-net
    source: community/modules/network/rdma-vpc
    settings:
      network_name: a3ultra-gke-rdma-net
      network_profile: https://www.googleapis.com/compute/staging_alpha/projects/$(vars.project_id)/global/networkProfiles/titanium-mrdma
      network_routing_mode: REGIONAL
      subnetworks_template:
        name_prefix: a3ultra-gke-mrdma-sub
        count: 8
        ip_range: 192.168.128.0/18
        region: $(vars.region)

  - id: a3-ultragpu-cluster
    source: modules/scheduler/gke-cluster
    use: [a3ultra-gke-net-0]
    settings:
      cluster_availability_type: MULTI_ZONAL
      cluster_reference_type: NAME
      min_master_version: 1.30.4-gke.1348000
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      zone: $(vars.zone)              # Temporarily used to target the GKE sandbox
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr)  # Allows your machine run kubectl command. It's required for the multi-network setup.
        display_name: "kubectl-access-network"
      additional_networks:
        $(concat(
          [{
            network=a3ultra-gke-net-1.network_name,
            subnetwork=a3ultra-gke-net-1.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
          a3ultra-gke-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: nccl_installer
    source: modules/management/kubectl-apply
    use: [a3-ultragpu-cluster]
    settings:
      apply_manifests:
      - source: /usr/local/google/home/annuay/nccl.yaml

  - id: a3-ultragpu-pool
    source: modules/compute/gke-node-pool
    use: [a3-ultragpu-cluster]
    settings:
      node_version: 1.30.4-gke.1348000    # Temporarily used to target the GKE sandbox
      machine_type: a3-ultragpu-8g
      zones: [$(vars.zone)]
      disk_type: hyperdisk-balanced
      guest_accelerator:
      - type: nvidia-h200-141gb
        count: 8
        gpu_driver_installation_config:
        - gpu_driver_version: "LATEST"
      additional_networks:
        $(concat(
          [{
          network=a3ultra-gke-net-1.network_name,
          subnetwork=a3ultra-gke-net-1.subnetwork_name,
            subnetwork_project=vars.project_id,
            nic_type="GVNIC",
            queue_count=null,
            network_ip=null,
            stack_type=null,
            access_config=[{nat_ip=null, public_ptr_domain_name=null, network_tier=null}],
            ipv6_access_config=[],
            alias_ip_range=[]
          }],
          a3ultra-gke-rdma-net.subnetwork_interfaces_gke
        ))
    outputs: [instructions]

  - id: job_template
    source: modules/compute/gke-job-template
    use: [a3-ultragpu-pool]
    settings:
      image: nvidia/cuda:11.0.3-runtime-ubuntu20.04
      command:
      - nvidia-smi
      node_count: 2
    outputs: [instructions]
