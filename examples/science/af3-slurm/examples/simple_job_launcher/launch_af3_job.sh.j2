#!/bin/bash
{#
Copyright 2026 "Google LLC"

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
#}


# --- Capture Launch Directory ---
LAUNCH_DIR=$(pwd)

# --- Script Defaults (Set via Ansible Variables) ---
DEFAULT_PARTITION_DATAPIPELINE="{{ datapipeline_partition }}"
DEFAULT_PARTITION_INFERENCE="{{ inference_partition }}"
DEFAULT_MEM_DATAPIPELINE="{{ datapipeline_memory }}"
DEFAULT_MEM_INFERENCE="{{ inference_memory }}"
DEFAULT_CPUS_DATAPIPELINE="{{ datapipeline_cpu_count }}"
DEFAULT_CPUS_INFERENCE="{{ inference_cpu_count }}"
DEFAULT_TIME_DATAPIPELINE="{{ datapipeline_timeout | default(3600) }}" # Expecting seconds
DEFAULT_TIME_INFERENCE="{{ inference_timeout | default(3600) }}"    # Expecting seconds
DEFAULT_GRES="{{ af3_default_gres | default('') }}"
DEFAULT_LOG_BASE_DIR="{{ af3_log_base_dir | default('$HOME/slurm_logs') }}"
DEFAULT_JOB_NAME_BASE="{{ af3_job_name_base | default('alphafold3') }}"

# --- AlphaFold 3 Science Variables (Set via Ansible Variables) ---
DEFAULT_MAX_TEMPLATE_DATE="{{ max_template_date }}"
DEFAULT_CONFORMER_MAX_ITERATIONS="{{ conformer_max_iterations }}"
DEFAULT_NUM_RECYCLES="{{ num_recycles }}"
DEFAULT_NUM_DIFFUSION_SAMPLES="{{ num_diffusion_samples }}"
DEFAULT_NUM_SEEDS="{{ num_seeds }}"
DEFAULT_SAVE_EMBEDDINGS="{{ save_embeddings }}"

# --- Fixed Configuration (Set via Ansible Variables) ---
CONTAINER_IMAGE="{{ sif_dir }}/af3.sif"
ALPHAFOLD_SCRIPT="/app/alphafold/run_alphafold.py"
MODEL_DIR="{{ model_dir }}"
DB_DIR="{{ db_dir }}"
PDB_DATABASE_PATH="{{ pdb_database_path }}"
DEFAULT_JAX_COMPILATION_CACHE_PATH="{{ jax_compilation_cache_path }}"

# --- Initialize variables ---
JOB_TYPE="" PARTITION="" MEM="" CPUS=""
TIME_SECONDS="" # Initialize as empty - will hold user input OR default
GRES="${DEFAULT_GRES}" LOG_BASE_DIR="${DEFAULT_LOG_BASE_DIR}"
JOB_NAME_BASE="${DEFAULT_JOB_NAME_BASE}" JAX_COMPILATION_CACHE_PATH="${DEFAULT_JAX_COMPILATION_CACHE_PATH}"
INPUT_PATH="" OUTPUT_DIR=""
GRES_USER_SET="false" # Flag to track if user provided --gres

# Initialize AF3 science variables with defaults
MAX_TEMPLATE_DATE="${DEFAULT_MAX_TEMPLATE_DATE}"
CONFORMER_MAX_ITERATIONS="${DEFAULT_CONFORMER_MAX_ITERATIONS}"
NUM_RECYCLES="${DEFAULT_NUM_RECYCLES}"
NUM_DIFFUSION_SAMPLES="${DEFAULT_NUM_DIFFUSION_SAMPLES}"
NUM_SEEDS="${DEFAULT_NUM_SEEDS}"
SAVE_EMBEDDINGS="${DEFAULT_SAVE_EMBEDDINGS}"


# --- Helper Functions ---
error_exit() { echo "Error: $1" >&2; usage >&2; exit 1; }

# Function to convert seconds to HH:MM:SS
format_time_hhmmss() {
  local total_seconds=$1
  if ! [[ "$total_seconds" =~ ^[0-9]+$ ]] || [ "$total_seconds" -lt 0 ]; then
    echo "Error: Invalid time value (seconds): '$total_seconds'. Must be a non-negative integer." >&2
    return 1 # Indicate failure
  fi
   if [ "$total_seconds" -eq 0 ]; then printf "00:00:00"; return 0; fi
  local ss=$((total_seconds % 60))
  local total_minutes=$((total_seconds / 60))
  local mm=$((total_minutes % 60))
  local hh=$((total_minutes / 60))
  printf "%02d:%02d:%02d" "$hh" "$mm" "$ss"
  return 0
}

# --- Function to Print Usage ---
usage() {
  # --- Pre-calculate display strings for defaults ---
  # Simple defaults (use directly in heredoc or pre-calculate if complex logic needed)
  local default_partition_datapipeline_disp="${DEFAULT_PARTITION_DATAPIPELINE}"
  local default_partition_inference_disp="${DEFAULT_PARTITION_INFERENCE}"
  local default_time_datapipeline_disp="${DEFAULT_TIME_DATAPIPELINE}"
  local default_time_inference_disp="${DEFAULT_TIME_INFERENCE}"
  local default_gres_disp="${DEFAULT_GRES:-gpu:1}" # Handle potential empty DEFAULT_GRES
  local default_job_name_base_disp="${DEFAULT_JOB_NAME_BASE}"
  local default_log_base_dir_disp="${DEFAULT_LOG_BASE_DIR}"

  # AF3 specific defaults - use :- expansion here to decide display text
  local default_jax_cache_disp="${DEFAULT_JAX_COMPILATION_CACHE_PATH:-using AF3 run_alphafold.py defaults}"
  local default_max_template_date_disp="${DEFAULT_MAX_TEMPLATE_DATE:-using AF3 run_alphafold.py defaults}"
  local default_conformer_iter_disp="${DEFAULT_CONFORMER_MAX_ITERATIONS:-using AF3 run_alphafold.py defaults}"
  local default_num_recycles_disp="${DEFAULT_NUM_RECYCLES:-using AF3 run_alphafold.py defaults}"
  local default_num_diffusion_disp="${DEFAULT_NUM_DIFFUSION_SAMPLES:-using AF3 run_alphafold.py defaults}"
  local default_num_seeds_disp="${DEFAULT_NUM_SEEDS:-using AF3 run_alphafold.py defaults}"
  local default_save_embeddings_disp="${DEFAULT_SAVE_EMBEDDINGS:-using AF3 run_alphafold.py defaults}"
  # --- End Pre-calculation ---

  cat << EOF
Usage: $0 --job-type <type> [OPTIONS] <input_path> <output_directory_path>

Submits an AlphaFold3 data pipeline OR inference job to Slurm.
Input and Output paths can be relative to the launch directory.

Required Arguments:
  --job-type TYPE           Job type: 'datapipeline' or 'inference'.
  <input_path>              Path to the input JSON file OR a directory containing input files.
  <output_directory_path>   Path to the desired output directory.

Options (override defaults):
  --partition PART                   Slurm partition (Defaults: datapipeline='${default_partition_datapipeline_disp}', inference='${default_partition_inference_disp}')
  --mem MEMORY                       Slurm memory request in GigaBytes (Default depends on job type)
  --cpus NUM                         Number of CPUs per task (Default depends on job type)
  --time SECONDS                     Slurm time limit in TOTAL SECONDS (e.g., 3600 for 1h).
                                     (Default depends on job type: '${default_time_datapipeline_disp}'s / '${default_time_inference_disp}'s)
  --gres SPEC                        Slurm GPU request.
                                     (Default: none for datapipeline, '${default_gres_disp}' for inference if unset)
  --job-name-base NAME               Base name for Slurm job (Default: '${default_job_name_base_disp}')
  --log-base-dir DIR                 Base directory for logs (Default: '${default_log_base_dir_disp}')

AlphaFold 3 Specific Options (Inference only):
  --jax-compilation-cache-path DIR   JAX cache directory (Inference only) (Default: ${default_jax_cache_disp})
  --max-template-date DATE           Maximum template date (YYYY-MM-DD) (Default: ${default_max_template_date_disp})
  --conformer-max-iterations NUM     Maximum conformer iterations (Default: ${default_conformer_iter_disp})
  --num-recycles NUM                 Number of recycles (Default: ${default_num_recycles_disp})
  --num-diffusion-samples NUM        Number of diffusion samples (Default: ${default_num_diffusion_disp})
  --num-seeds NUM                    Number of seeds (Default: ${default_num_seeds_disp})
  --save-embeddings VALUE            Save embeddings ('true'/'false') (Default: ${default_save_embeddings_disp})

  -h, --help                         Display this help message and exit.

Examples:
  # Data pipeline job, time override in seconds
  $0 --job-type datapipeline --time 7200 input.json output_pipe/ # 7200 seconds = 2 hours
  # Inference job (uses default GRES & time if not overridden) with AF3 options
  $0 --job-type inference --mem 64G --num-recycles 6 --num-seeds 3 input.json output_infer/
EOF
}

# --- Argument Parsing ---
while [[ $# -gt 0 ]]; do
  key="$1"
  case $key in
    --job-type)       JOB_TYPE="$2"; shift 2 ;;
    --partition)      PARTITION="$2"; shift 2 ;;
    --time)
        if ! [[ "$2" =~ ^[0-9]+$ ]]; then error_exit "Invalid format for --time: '$2'. Expected total seconds (integer)."; fi
        TIME_SECONDS="$2"; shift 2 ;;
    --mem)            MEM="$2"; shift 2 ;;
    --cpus)           CPUS="$2"; shift 2 ;;
    --gres)           GRES="$2"; GRES_USER_SET="true"; shift 2 ;;
    --job-name-base)  JOB_NAME_BASE="$2"; shift 2 ;;
    --log-base-dir)   LOG_BASE_DIR="$2"; shift 2 ;;
    --jax-compilation-cache-path)  JAX_COMPILATION_CACHE_PATH="$2"; shift 2 ;;
    # --- NEW AF3 ARG PARSING ---
    --max-template-date)          MAX_TEMPLATE_DATE="$2"; shift 2 ;;
    --conformer-max-iterations)   CONFORMER_MAX_ITERATIONS="$2"; shift 2 ;;
    --num-recycles)               NUM_RECYCLES="$2"; shift 2 ;;
    --num-diffusion-samples)      NUM_DIFFUSION_SAMPLES="$2"; shift 2 ;;
    --num-seeds)                  NUM_SEEDS="$2"; shift 2 ;;
    --save-embeddings)            SAVE_EMBEDDINGS="$2"; shift 2 ;;
    # --- END NEW AF3 ARG PARSING ---
    -h|--help)        usage; exit 0 ;;
    -*)               error_exit "Unknown option: $1" ;;
    *) # Positional arguments
      if [ -z "$INPUT_PATH" ]; then INPUT_PATH="$1";
      elif [ -z "$OUTPUT_DIR" ]; then OUTPUT_DIR="$1";
      else error_exit "Unexpected argument: $1"; fi
      shift ;;
  esac
done


# --- Input Validation & Path Resolution ---
[ -z "$JOB_TYPE" ] && error_exit "--job-type is required ('datapipeline' or 'inference')."
[[ "$JOB_TYPE" != "datapipeline" && "$JOB_TYPE" != "inference" ]] && \
    error_exit "Invalid --job-type: '$JOB_TYPE'. Must be 'datapipeline' or 'inference'."
[ -z "$INPUT_PATH" ] || [ -z "$OUTPUT_DIR" ] && \
    error_exit "Missing required arguments: input_path and output_directory_path."

original_input_path="$INPUT_PATH"
if [[ ! "$INPUT_PATH" == /* ]]; then INPUT_PATH="${LAUNCH_DIR}/${INPUT_PATH}"; fi
INPUT_PATH=$(readlink -m "$INPUT_PATH")
if [ $? -ne 0 ] || [ -z "$INPUT_PATH" ]; then error_exit "Could not resolve absolute path for input path '$original_input_path'."; fi
echo "Info: Using absolute input path: $INPUT_PATH"
if [ ! -f "$INPUT_PATH" ] && [ ! -d "$INPUT_PATH" ]; then error_exit "Resolved input path '$INPUT_PATH' not found or is not a regular file or directory."; fi

original_output_path="$OUTPUT_DIR"
if [[ ! "$OUTPUT_DIR" == /* ]]; then OUTPUT_DIR="${LAUNCH_DIR}/${OUTPUT_DIR}"; fi
OUTPUT_DIR=$(readlink -m "$OUTPUT_DIR")
if [ $? -ne 0 ] || [ -z "$OUTPUT_DIR" ]; then error_exit "Could not resolve absolute path for output directory '$original_output_path'."; fi
echo "Info: Using absolute output directory path: $OUTPUT_DIR"

if [ -d "$INPUT_PATH" ]; then python_input_flag="--input_dir"; else python_input_flag="--json_path"; fi
python_input_value="$INPUT_PATH"


# --- Set Defaults Based on Job Type (if not set by user) ---
if [ "$JOB_TYPE" = "datapipeline" ]; then
    [ -z "$PARTITION" ] && PARTITION="$DEFAULT_PARTITION_DATAPIPELINE"
    [ -z "$MEM" ]       && MEM="$DEFAULT_MEM_DATAPIPELINE"
    [ -z "$CPUS" ]      && CPUS="$DEFAULT_CPUS_DATAPIPELINE"
    [ -z "$TIME_SECONDS" ] && TIME_SECONDS="$DEFAULT_TIME_DATAPIPELINE"
elif [ "$JOB_TYPE" = "inference" ]; then
    [ -z "$PARTITION" ] && PARTITION="$DEFAULT_PARTITION_INFERENCE"
    [ -z "$MEM" ]       && MEM="$DEFAULT_MEM_INFERENCE"
    [ -z "$CPUS" ]      && CPUS="$DEFAULT_CPUS_INFERENCE"
    [ -z "$TIME_SECONDS" ] && TIME_SECONDS="$DEFAULT_TIME_INFERENCE"
    # Set default GRES for inference only if user did not specify --gres
    if [ "$GRES_USER_SET" = "false" ] && [ -n "${DEFAULT_GRES}" ]; then
        GRES="${DEFAULT_GRES}" # Use Ansible default GRES if user didn't provide one
        echo "Info: Using default GRES for inference from config: ${GRES}"
    elif [ "$GRES_USER_SET" = "false" ]; then
         GRES="gpu:1" # Fallback default GRES if Ansible default is empty
         echo "Info: Using fallback default GRES for inference: ${GRES}"
    fi
fi


# --- Format Final Time Value ---
TIME_HMS=$(format_time_hhmmss "$TIME_SECONDS")
if [ $? -ne 0 ]; then
    error_exit "Invalid time value obtained: $TIME_SECONDS seconds."
fi


# --- Job Type Specific Configuration ---
declare -a apptainer_binds=()
apptainer_nv_flag="" run_data_pipeline_flag="" run_inference_flag=""
base_python_opts="" # Base options for both types
job_specific_python_opts="" # Options specific to job type

apptainer_binds+=("--bind" "$(dirname "${INPUT_PATH}")")
apptainer_binds+=("--bind" "${OUTPUT_DIR}")

# Construct base python options applicable to both (if any in future)
# base_python_opts+="..."

if [ "$JOB_TYPE" = "datapipeline" ]; then
    final_job_name="${JOB_NAME_BASE}_datapipeline"
    run_data_pipeline_flag="--run_data_pipeline=True"; run_inference_flag="--run_inference=False"
    job_specific_python_opts="--jackhmmer_n_cpu=${CPUS} --nhmmer_n_cpu=${CPUS}"
    apptainer_binds+=("--bind" "${DB_DIR}"); apptainer_binds+=("--bind" "${PDB_DATABASE_PATH}")
elif [ "$JOB_TYPE" = "inference" ]; then
    final_job_name="${JOB_NAME_BASE}_inference"
    run_data_pipeline_flag="--run_data_pipeline=False"; run_inference_flag="--run_inference=True"
    [ -n "$GRES" ] && apptainer_nv_flag="--nv"
    apptainer_binds+=("--bind" "${MODEL_DIR}")

    # Add JAX cache path if set
    if [ -n "$JAX_COMPILATION_CACHE_PATH" ]; then
        job_specific_python_opts+=" --jax_compilation_cache_path='${JAX_COMPILATION_CACHE_PATH}'"
        apptainer_binds+=("--bind" "${JAX_COMPILATION_CACHE_PATH}")
        echo "Info: JAX cache enabled at ${JAX_COMPILATION_CACHE_PATH}"
    else
        echo "Info: JAX cache directory not set, JAX caching disabled."
    fi

    # --- Add AF3 science python options conditionally ---
    af3_extra_opts=""
    [ -n "$MAX_TEMPLATE_DATE" ] && af3_extra_opts+=" --max_template_date=\"${MAX_TEMPLATE_DATE}\""
    [ -n "$CONFORMER_MAX_ITERATIONS" ] && af3_extra_opts+=" --conformer_max_iterations=${CONFORMER_MAX_ITERATIONS}"
    [ -n "$NUM_RECYCLES" ] && af3_extra_opts+=" --num_recycles=${NUM_RECYCLES}"
    [ -n "$NUM_DIFFUSION_SAMPLES" ] && af3_extra_opts+=" --num_diffusion_samples=${NUM_DIFFUSION_SAMPLES}"
    [ -n "$NUM_SEEDS" ] && af3_extra_opts+=" --num_seeds=${NUM_SEEDS}"
    [ -n "$SAVE_EMBEDDINGS" ] && af3_extra_opts+=" --save_embeddings=${SAVE_EMBEDDINGS}"

    job_specific_python_opts+="${af3_extra_opts}" # Append AF3 specific options
fi


# --- Create Log Dir & Pre-submission Checks ---
mkdir -p "$LOG_BASE_DIR" || echo "Warning: Could not create log base dir '$LOG_BASE_DIR'." >&2
[ ! -f "${CONTAINER_IMAGE}" ] && { echo "Error: Container image '${CONTAINER_IMAGE}' not found." >&2; exit 1; }


# --- Assemble sbatch Command ---
SBATCH_DIRECTIVES=$(cat << SBD
#!/bin/bash
#SBATCH --job-name=${final_job_name}
#SBATCH --partition=${PARTITION}
#SBATCH --time=${TIME_HMS}
#SBATCH --mem=${MEM}G
#SBATCH --cpus-per-task=${CPUS}
#SBATCH --output=${LOG_BASE_DIR}/job_%j/out.txt
#SBATCH --error=${LOG_BASE_DIR}/job_%j/err.txt
SBD
)
# Only add --gres directive if GRES is non-empty
[ -n "$GRES" ] && SBATCH_DIRECTIVES+=$'\n'"#SBATCH --gres=${GRES}"


# --- Submit sbatch Command ---
bind_args_str=$(printf "%s " "${apptainer_binds[@]}")
echo "Submitting job ${final_job_name} to partition ${PARTITION}..."
echo "Time Limit:       ${TIME_HMS} (${TIME_SECONDS} seconds)"
echo "Memory:           ${MEM}G"
echo "CPUs:             ${CPUS}"
[ -n "$GRES" ] && echo "GRES:             ${GRES}" || echo "GRES:             None"
echo "Log File Pattern: ${LOG_BASE_DIR}/job_%j/{out.txt,err.txt}"

# Combine all python options
all_python_opts="${base_python_opts} ${job_specific_python_opts}"

sbatch << EOF
${SBATCH_DIRECTIVES}

# --- Job Environment & Setup ---
echo "--- Job Info ---"
echo "Job Type: $JOB_TYPE"
echo "ID: \$SLURM_JOB_ID (\${final_job_name}), Host: \$(hostname), Partition: \${SLURM_JOB_PARTITION}"
echo "Allocated CPUs: \$SLURM_CPUS_PER_TASK, Mem: \$SLURM_MEM_PER_NODE G"
[ -n "\$SLURM_JOB_GPUS" ] && echo "Allocated GPUs: \$SLURM_JOB_GPUS (\`nvidia-smi -L 2>/dev/null | grep \${CUDA_VISIBLE_DEVICES:-NONE} || echo 'GPU details unavailable'\`)" || echo "Allocated GPUs: None"
echo "Input Path (${python_input_flag}): ${python_input_value}"
echo "Output Dir: ${OUTPUT_DIR}"
if [ "$JOB_TYPE" = "inference" ]; then
    JAX_CACHE_DIR_ACTUAL="${JAX_COMPILATION_CACHE_PATH:-}" # Use shell parameter expansion for safety
    if [ -n "\${JAX_CACHE_DIR_ACTUAL}" ]; then
        echo "JAX Cache: \${JAX_CACHE_DIR_ACTUAL}"
        mkdir -p "\${JAX_CACHE_DIR_ACTUAL}" # Ensure JAX cache dir exists on compute node
        export XLA_FLAGS="--xla_gpu_force_compilation_cache_path=\${JAX_CACHE_DIR_ACTUAL}"
        export JAX_CACHE_DIR="\${JAX_CACHE_DIR_ACTUAL}" # Also set JAX_CACHE_DIR just in case
        echo "Set XLA_FLAGS: \$XLA_FLAGS"
        echo "Set JAX_CACHE_DIR: \$JAX_CACHE_DIR"
    else
        echo "JAX Cache: Disabled (JAX_COMPILATION_CACHE_PATH not set)"
    fi
    # Print AF3 specific options used
    echo "--- AlphaFold 3 Inference Options ---"
    echo "Max Template Date: ${MAX_TEMPLATE_DATE:-AF3 default}"
    echo "Conformer Max Iterations: ${CONFORMER_MAX_ITERATIONS:-AF3 default}"
    echo "Num Recycles: ${NUM_RECYCLES:-AF3 default}"
    echo "Num Diffusion Samples: ${NUM_DIFFUSION_SAMPLES:-AF3 default}"
    echo "Num Seeds: ${NUM_SEEDS:-AF3 default}"
    echo "Save Embeddings: ${SAVE_EMBEDDINGS:-AF3 default}"
    echo "-------------------------------------"
fi
echo "--- Start: \$(date) ---"
mkdir -p "${OUTPUT_DIR}"

# --- Execute the AlphaFold Command ---
echo "Launching AlphaFold ($JOB_TYPE)..."
set -e # Exit on error
set -x # Print command being executed

apptainer run ${apptainer_nv_flag} \\
    ${bind_args_str} \\
  "${CONTAINER_IMAGE}" python3 "${ALPHAFOLD_SCRIPT}" \\
    ${python_input_flag} "${python_input_value}" \\
    --output_dir "${OUTPUT_DIR}" \\
    ${run_data_pipeline_flag} \\
    ${run_inference_flag} \\
    --model_dir="${MODEL_DIR}" \\
    --db_dir="${DB_DIR}" \\
    --pdb_database_path="${PDB_DATABASE_PATH}" \\
    ${all_python_opts}

set +x
echo "--- End: \$(date), Exit Code: \$? ---"
exit 0
EOF

# --- Post-submission Check ---
if [ $? -eq 0 ]; then
    echo "Job successfully submitted (Job ID printed above by sbatch)."
else
    echo "Error submitting job to Slurm." >&2
    exit 1
fi

exit 0
