# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import requests
import os
from typing import Optional, Dict, List
import jinja2
import datetime
import pathlib
import ijson
import string


def is_valid_path(input_path: str, expected_prefix: str) -> bool:
    input_path = os.path.abspath(input_path)
    return os.path.isabs(input_path) and input_path.startswith(expected_prefix)


def sanitise_name(name: str) -> str:
    """Returns sanitised version of the name that can be used as a filename."""
    lower_spaceless_name = name.lower().replace(' ', '_')
    allowed_chars = set(string.ascii_lowercase + string.digits + '_-.')
    return ''.join(l for l in lower_spaceless_name if l in allowed_chars)


def retrieve_sanitised_sequence_name(input_file: str) -> Optional[List[str]]:
    """Retrieves a list of sanitised sequence name from given input file."""
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Input file '{input_file}' does not exist.")
    try:
        with open(input_file, 'r', encoding="utf-8") as f:
            parser = ijson.items(f, '')
            try:
                root = next(parser)
            except StopIteration:
                raise ValueError("Empty JSON file.")

            if not isinstance(root, list) and not isinstance(root, dict):
                raise ValueError(
                    "[ERROR] Top-level JSON must be a list or a dictionary.")

            items: Optional[List[Dict]] = None
            if isinstance(root, list):
                if len(root) <= 0:
                    raise ValueError(
                        "[ERROR] The AlphaFold Server format is not supported: the list must contain at least one dictionary item.")
                items = root

            # Case 2: root is a dict
            elif isinstance(root, dict):
                items = [root]

            # Validate and extract 'name'
            sanitised_name_list = []
            for item in items:
                if not isinstance(item, dict):
                    raise ValueError(
                        "Item in Alphafold Format must be a dictionary.")
                if 'name' not in item:
                    raise ValueError(
                        "'name' key must exist in the dictionary.")
                name = item.get('name')
                if not isinstance(name, str) or not name:
                    raise ValueError(
                        "'name' key must exist, contains value and be a string.")
                sanitised_name_list.append(sanitise_name(name))
            return sanitised_name_list
    except FileNotFoundError:
        print(f"[ERROR] Input file '{input_file}' not found.")
        raise FileNotFoundError(
            f"[ERROR] Input file '{input_file}' not found.")
    except Exception as e:
        print(f"[ERROR] Error processing input file '{input_file}': {e}")
        raise Exception(
            f"[ERROR] Error processing input file '{input_file}': {e}")


class AF3SlurmClient:
    """An AF3 client for interacting with the Slurm API."""
    SLURM_API_VERSION = "v0.0.41"
    PARTITION_CONFIG_MAP: Dict[str, Dict] = {
        "{{ datapipeline_c3dhm_partition_name}}": {
            "cpu_count": {{datapipeline_c3dhm_partition_cpu_count}},
            "memory": "{{ datapipeline_c3dhm_partition_memory }}G"
        },
        "{{ inference_g2_partition_name }}": {
            "cpu_count": {{inference_g2_partition_cpu_count}},
            "memory": "{{ inference_g2_partition_memory }}G"
        },
        "{{ inference_a2_partition_name }}": {
            "cpu_count": {{inference_a2_partition_memory}},
            "memory": "{{ datapipeline_c3dhm_partition_memory }}G"
        },
        "{{ inference_a2u_partition_name }}": {
            "cpu_count": {{inference_a2u_partition_cpu_count}},
            "memory": "{{ inference_a2u_partition_memory }}G"
        },

    }

    BASE_ENVIRONMENT_LIST: List[str] = [
        "PATH=/bin:/usr/bin/:/usr/local/bin/",
        "LD_LIBRARY_PATH=/lib/:/lib64/:/usr/local/lib",
    ]

    def __init__(self, hostname: str, port: int, token: str, science_settings: Dict) -> None:

        self.slurm_base_url = f"http://{hostname}:{port}/slurm/{self.SLURM_API_VERSION}"
        self.slurmdb_base_url = f"http://{hostname}:{port}/slurmdb/{self.SLURM_API_VERSION}"
        self.token = token

        # Af3 default config
        self.af3_config = {
            "datapipeline_timeout": "{{ default_datapipeline_timeout }}",
            "inference_timeout": "{{ default_inference_timeout }}",
            "jax_compilation_cache_path": "{{ jax_compilation_cache_path }}",
            "max_template_date": science_settings.get("max_template_date", ""),
            "conformer_max_iterations": science_settings.get("conformer_max_iterations", ""),
            "num_recycles": science_settings.get("num_recycles", ""),
            "num_diffusion_samples": science_settings.get("num_diffusion_samples", ""),
            "num_seeds": science_settings.get("num_seeds", ""),
            "save_embeddings": science_settings.get("save_embeddings", ""),

            # Warning: don't change manually
            "job_name": "af3_slurm_restapi",
            "slurm_node_working_directory": "{{ slurm_node_working_directory }}",
            "service_user": "{{ service_user }}",
            "sif_dir": "{{ sif_dir }}",
            "model_dir": "{{ model_dir }}",
            "db_dir": "{{ db_dir }}",
            "pdb_database_path": "{{ pdb_database_path }}",
            "job_template_path": "{{ af3_job_template_jupyter_path }}",
            "jupyter_working_directory": "{{ af3ipynb_bucket_local_mount }}",
            "bucket_name": "{{ af3ipynb_bucket }}"
        }

    def __get_headers(self) -> Optional[Dict]:
        return {"X-SLURM-USER-TOKEN": self.token}

    def __retrieve_slurm_url(self, endpoint: str) -> str:
        return f"{self.slurm_base_url}/{endpoint}"

    def __retrieve_slurmdb_url(self, endpoint: str) -> str:
        return f"{self.slurmdb_base_url}/{endpoint}"

    def ping(self) -> Optional[requests.Response]:
        """Pings the Slurm API server."""
        url = self.__retrieve_slurm_url("ping")
        try:
            headers = self.__get_headers()
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] pinging Slurm API: {e}")
            raise requests.exceptions.RequestException(
                f"[ERROR] pinging Slurm API: {e}")

    def _render_template(self, config_options: Dict) -> str:
        """Renders the Jinja2 template with af3_config values."""
        env = jinja2.Environment(loader=jinja2.FileSystemLoader(
            os.path.dirname(self.af3_config["job_template_path"])))
        template = env.get_template(
            os.path.basename(self.af3_config["job_template_path"]))
        rendered = template.render(config_options)
        return rendered

    def submit_job(self, job_config: Dict, script: str) -> Optional[Dict]:
        """Submits a job to Slurm server."""
        url = self.__retrieve_slurm_url("job/submit")
        submit_input = {"job": job_config, "script": script}
        try:
            headers = self.__get_headers()
            response = requests.post(url, headers=headers, json=submit_input)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            error_message = f"[ERROR] Failed to submit job to Slurm: {str(e)}"
            print(error_message)
            raise RuntimeError(error_message) from e

    def submit_base_job(self, job_config: Dict, job_type: str, script_options: Dict, input_file: str, output_dir: Optional[str] = None) -> Optional[Dict]:
        """Submits a data pipeline job to Slurm server."""

        updated_script_options = {**self.af3_config, **script_options, ** {
            "remote_input_path": input_file,
            "remote_output_dir": output_dir,
            "remote_bucket_name": self.af3_config["bucket_name"],
            "input_path": os.path.join(self.af3_config["slurm_node_working_directory"], output_dir, "input_data", input_file),
            "output_dir": os.path.join(self.af3_config["slurm_node_working_directory"], output_dir),
            "job_type": job_type,
            "af3_log_base_dir": os.path.join(output_dir, "slurm_logs"),
        }}
        updated_job_config = {**job_config, ** {
            "standard_output": os.path.join(updated_script_options["af3_log_base_dir"], "job_%j", "out.txt"),
            "standard_error": os.path.join(updated_script_options["af3_log_base_dir"], "job_%j", "err.txt"),
        }}
        script = self._render_template(updated_script_options)
        submit_result = self.submit_job(updated_job_config, script)
        print("[INFO] Please wait for the job until finished to see the result output.")
        return submit_result

    def submit_inference_job(self, input_file: str, partition_name: str, enable_unified_memory: Optional[bool] = None, output_dir: Optional[str] = None) -> Optional[Dict]:
        """Submits an inference job to Slurm server."""

        if not input_file:
            raise ValueError("Input file must be provided.")

        partition_config = self.PARTITION_CONFIG_MAP.get(partition_name, {})
        if not partition_config:
            raise ValueError(
                f"Invalid partition name: {partition_name}, should be one of {list(self.PARTITION_CONFIG_MAP.keys())}"
            )

        if enable_unified_memory is None:
            # Default to False if not provided
            enable_unified_memory = False

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        job_type = "inference"
        if not output_dir:
            # Generate a timestamped output path if not provided
            file_name = pathlib.Path(input_file).stem
            output_dir = os.path.join(
                f"{job_type}_result", file_name, f"{timestamp}")

        job_config = {
            "account": self.af3_config["service_user"],
            "tasks": 1,
            "name": self.af3_config["job_name"],
            "partition": partition_name,
            "current_working_directory": self.af3_config["slurm_node_working_directory"],
            "time_limit": self.af3_config["inference_timeout"],
            "memory_per_node": partition_config.get("memory", "4G"),
            "cpus_per_task": partition_config.get("cpu_count", 1),
            "tres_per_job": "gres/gpu:1",  # Enables GPU
            "environment": self.BASE_ENVIRONMENT_LIST,
        }
        script_options = {
            "inference_partition": partition_name,  # Inference partition name
            # Inference memory
            "inference_memory": partition_config.get("memory", "4G"),
            # Inference CPU count
            "inference_cpu_count":  str(partition_config.get('cpu_count', 1)),
            "inference_enable_unified_memory": str(enable_unified_memory).lower(),
        }
        response = self.submit_base_job(
            job_config, job_type, script_options, input_file, output_dir)

        sanitised_sequence_name_list: Optional[List[str]] = retrieve_sanitised_sequence_name(
            input_file=input_file
        )
        output_path_list: List[Dict] = []

        for sanitised_sequence_name in sanitised_sequence_name_list:

            print(
                f"[INFO] Inference output path for sequence: {input_file}")
            inference_model_cif_output_path = os.path.join(
                output_dir, f"{sanitised_sequence_name}", f"{sanitised_sequence_name}_model.cif")
            inference_summary_confidences_output_path = os.path.join(
                output_dir, f"{sanitised_sequence_name}", f"{sanitised_sequence_name}_summary_confidences.json")
            inference_confidences_output_path = os.path.join(
                output_dir, f"{sanitised_sequence_name}", f"{sanitised_sequence_name}_confidences.json")
            output_path_list.append({
                "model_cif": inference_model_cif_output_path,
                "summary_confidences": inference_summary_confidences_output_path,
                "confidences": inference_confidences_output_path
            })
            print(
                f"         ├── Model CIF output path           : {inference_model_cif_output_path}")
            print(
                f"         ├── Confidences output path         : {inference_confidences_output_path}")
            print(
                f"         └── Summary confidences output path : {inference_summary_confidences_output_path}")
        response["output_path_list"] = output_path_list
        return response

    def submit_data_pipeline_job(self,  input_file: str, partition_name: str, output_dir: Optional[str] = None) -> Optional[Dict]:
        """Submits a data pipeline job to Slurm server."""

        if not input_file:
            raise ValueError("Input file must be provided.")

        partition_config = self.PARTITION_CONFIG_MAP.get(partition_name, {})
        if not partition_config:
            raise ValueError(
                f"Invalid partition name: {partition_name}, should be one of {list(self.PARTITION_CONFIG_MAP.keys())}")
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        job_type = "datapipeline"
        if not output_dir:
            # Generate a timestamped output path if not provided
            file_name = pathlib.Path(input_file).stem
            output_dir = os.path.join(
                f"{job_type}_result",  file_name, f"{timestamp}"
            )

        job_config = {
            "account": self.af3_config["service_user"],
            "tasks": 1,
            "name": self.af3_config["job_name"],
            "partition": partition_name,
            "current_working_directory": self.af3_config["slurm_node_working_directory"],
            "time_limit": self.af3_config["inference_timeout"],
            "memory_per_node": partition_config.get("memory", "4G"),
            "cpus_per_task": partition_config.get("cpu_count", 1),
            "environment":  self.BASE_ENVIRONMENT_LIST,
        }
        script_options = {
            "datapipeline_partition": partition_name,  # Datapipeline partition name
            # Datapipeline memory
            "datapipeline_memory": partition_config.get("memory", "4G"),
            # Datapipeline CPU count
            "datapipeline_cpu_count":  partition_config.get('cpu_count', 1),
        }
        response = self.submit_base_job(
            job_config, job_type, script_options, input_file, output_dir
        )
        sanitised_sequence_name_list = retrieve_sanitised_sequence_name(
            input_file)
        output_path_list: List[Dict] = []

        for sanitised_sequence_name in sanitised_sequence_name_list:
            print(
                f"[INFO] Datapipeline output path for sequence: {sanitised_sequence_name}")
            datapipeline_output_file_path = os.path.join(
                output_dir, f"{sanitised_sequence_name}", f"{sanitised_sequence_name}_data.json")
            output_path_list.append({
                "output_path": datapipeline_output_file_path
            })
            print(
                f"         └── Path: {datapipeline_output_file_path}")

        response["output_path_list"] = output_path_list
        return response

    def cancel_job(self, job_id: int) -> Optional[Dict]:
        """Cancels a job on the Slurm server."""
        url = self.__retrieve_slurm_url(f"job/{job_id}/cancel")
        try:
            headers = self.__get_headers()
            response = requests.post(url, headers=headers)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] canceling job {job_id} on Slurm: {e}")
            raise requests.exceptions.RequestException(
                f"[ERROR] canceling job {job_id} on Slurm: {e}")

    def get_job_info(self, job_id: int) -> Optional[Dict]:
        """Retrieves information about a specific job."""
        url = self.__retrieve_slurmdb_url(f"job/{job_id}")
        try:
            headers = self.__get_headers()
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] retrieving job {job_id} from Slurm: {e}")
            raise requests.exceptions.RequestException(
                f"[ERROR] retrieving job {job_id} from Slurm: {e}")

    def get_job_state(self, job_id: int) -> Optional[str]:
        """Retrieves information about a specific job."""
        job_info = self.get_job_info(job_id)
        if not job_info:
            return None
        job_state = job_info.get("jobs", [])[0].get(
            "state", {}).get("current", [])[0]
        return job_state

    def get_all_jobs(self) -> Optional[Dict]:
        """Retrieves information about all jobs."""
        url = self.__retrieve_slurmdb_url("jobs")
        try:
            headers = self.__get_headers()
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"[ERROR] retrieving all jobs from Slurm: {e}")
            return requests.exceptions.RequestException(
                f"[ERROR] retrieving all jobs from Slurm: {e}")
