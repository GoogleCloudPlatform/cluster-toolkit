# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---

blueprint_name: gke-a3-mega-tii-poc

# GCS bucket for persisting the Terraform state
terraform_backend_defaults:
  type: gcs
  configuration:
    bucket: <<GCS_BUCKET_NAME>> # tii-poc-gke-a3mega-bucket

vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: gke-a3-mega-tii-poc
  region: us-west1
  zone: us-west1-a

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr: <your-ip-address>/32

deployment_groups:
- group: setup
  modules:
  - id: network1
    source: modules/network/vpc
    settings:
      subnetwork_name: gke-subnet-a3-mega-tii-poc
      mtu: 8896
      default_primary_subnetwork_size: 13 # This is CIDR /22, up to 1020 nodes -> cidrsubnet(var.network_address_range, var.default_primary_subnetwork_size, 0) -> 9+13=22
      secondary_ranges:
        gke-subnet-a3-mega-tii-poc:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: private_service_access # required for parallelstore
    source: community/modules/network/private-service-access
    use: [network1]
    settings:
      prefix_length: 24


- group: primary
  modules:
  # allow parallelstore connection
  - id: parallelstore_firewall_rule
    source: modules/network/firewall-rules
    use: [network1]
    settings:
      ingress_rules:
      - name: $(vars.deployment_name)-allow-parallelstore-traffic
        description: Allow parallelstore traffic
        source_ranges:
        - $(private_service_access.cidr_range)
        allow:
        - protocol: tcp

  - id: gke_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectAdmin
      - parallelstore.admin
      - artifactregistry.reader

  - id: gpunets
    source: modules/network/multivpc
    settings:
      network_name_prefix: $(vars.deployment_name)-gpunet
      global_ip_address_range: 192.169.0.0/16
      network_count: 8
      subnetwork_cidr_suffix: 20
      mtu: 8244

  - id: gke_cluster
    source: modules/scheduler/gke-cluster
    use: [network1, gpunets, gke_service_account]
    settings:
      min_master_version: 1.30.4-gke.1348000
      enable_gcsfuse_csi: true
      enable_parallelstore_csi: true
      enable_dcgm_monitoring: true
      enable_node_local_dns_cache: true
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr)  # Allows your machine run kubectl command. It's required for the multi-network setup.
        display_name: "kubectl-access-network"
    outputs: [instructions]

  ### Set up storage class and persistent volume claim for Parallelstore ###
  - id: parallelstore-setup
    source: modules/file-system/gke-storage
    use: [gke_cluster, private_service_access]
    settings:
      storage_type: Parallelstore
      access_mode: ReadWriteMany
      sc_volume_binding_mode: Immediate
      sc_reclaim_policy: Retain # Use Retain if you want to volume and parallelstore resource will remain after
      sc_topology_zones: [$(vars.zone)]
      pvc_count: 1
      capacity_gb: 100000 # from 12,000 GiB to 100,000 GiB, in multiples of 4,000 GiB

  - id: default_pool # default node pool for non GPU workload
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gke_service_account]
    settings:
      name: default-pool
      machine_type: e2-standard-16
      autoscaling_total_min_nodes: 2
      zones: [$(vars.zone)]
    outputs: [instructions]

  - id: topology_aware_scheduler_install
    source: community/modules/compute/gke-topology-scheduler
    use: [gke_cluster]

  - id: a3_megagpu_pool_1
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gpunets, gke_service_account]
    settings:
      name: a3mega-np-1
      machine_type: a3-megagpu-8g
      static_node_count: 512
      disk_size_gb: 200
      reservation_affinity:
        consume_reservation_type: SPECIFIC_RESERVATION
        specific_reservations:
        - name: <<RESERVATION_NAME>> # tii-gke-poc
          project: $(vars.project_id)
      zones: [$(vars.zone)]
    outputs: [instructions]

  - id: workload_manager_install
    source: modules/management/kubectl-apply
    use: [gke_cluster]
    settings:
      kueue:
        install: true
      jobset:
        install: true
