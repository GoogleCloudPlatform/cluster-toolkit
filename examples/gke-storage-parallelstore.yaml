# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
blueprint_name: gke-storage-parallelstore
vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: gke-storage-parallelstore
  region: us-central1
  zone: us-central1-c

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr: <your-ip-address>/32

deployment_groups:
- group: primary
  modules:
  - id: network
    source: modules/network/vpc
    settings:
      subnetwork_name: gke-subnet-parallelstore
      secondary_ranges:
        gke-subnet-parallelstore:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: private_service_access # required for parallelstore
    source: community/modules/network/private-service-access
    use: [network]
    settings:
      prefix_length: 24

  - id: gke_cluster
    source: modules/scheduler/gke-cluster
    use: [network]
    settings:
      enable_parallelstore_csi: true # enable Parallelstore for the cluster
      configure_workload_identity_sa: true
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      master_authorized_networks:
      - display_name: deployment-machine
        cidr_block: $(vars.authorized_cidr)
    outputs: [instructions]

  ### Set up storage class and persistent volume claim for Parallelstore ###
  - id: parallelstore-setup
    source: modules/file-system/gke-storage
    use: [gke_cluster, private_service_access]
    settings:
      storage_type: Parallelstore
      access_mode: ReadWriteMany
      sc_volume_binding_mode: Immediate
      sc_reclaim_policy: Delete # Use Retain if you want to volume and parallelstore resource will remain after
      sc_topology_zones: [$(vars.zone)]
      pvc_count: 2
      capacity_gb: 12000 # from 12,000 GiB to 100,000 GiB, in multiples of 4,000 GiB

  - id: sample-pool
    source: modules/compute/gke-node-pool
    use: [gke_cluster]
    settings:
      name: sample-pool
      zones: [$(vars.zone)]
      machine_type: n2-standard-4

  ### Parallelstore enabled Job ###

  - id: parallelstore-job
    source: modules/compute/gke-job-template
    use:
    - gke_cluster
    - parallelstore-setup
    settings:
      image: busybox
      command:
      - bin/sh
      - -c
      - |
        echo "Set up job folders"
        shopt -s extglob; JOB=${HOSTNAME%%-+([[:digit:]])}
        mkdir /data/parallelstore-pvc-0/${JOB}/ -p;
        mkdir /data/parallelstore-pvc-1/${JOB}/ -p;

        echo "Writing seed data to Parallelstore volumes"
        dd if=/dev/urandom of=/data/parallelstore-pvc-0/${JOB}/${JOB_COMPLETION_INDEX}.dat bs=1K count=1000
        dd if=/dev/urandom of=/data/parallelstore-pvc-1/${JOB}/${JOB_COMPLETION_INDEX}.dat bs=1K count=1000

        # echo "Hash file and write between the 2 hyerpdisk balanced volumes"
        # md5sum /data/parallelstore-pvc-0/${JOB}/${JOB_COMPLETION_INDEX}.dat > /data/parallelstore-pvc-1/${JOB}/${JOB_COMPLETION_INDEX}.md5
        # md5sum /data/parallelstore-pvc-1/${JOB}/${JOB_COMPLETION_INDEX}.dat > /data/parallelstore-pvc-0/${JOB}/${JOB_COMPLETION_INDEX}.md5
      node_count: 5
    outputs: [instructions]
