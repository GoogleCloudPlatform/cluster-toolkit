# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---

blueprint_name: gke-a3-mega

vars:
  # The following variables should be over-written in the deployment.yaml file.
  # Your GCP Project ID
  project_id: ## Set GCP Project ID Here ##

  # This should be unique across all of your Cluster
  # Toolkit Deployments.
  deployment_name: gke-a3-mega

  # The GCP Region used for this deployment.
  region: us-central1

  # The GCP Zone used for this deployment.
  zone: us-central1-c

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr:

  gcp_public_cidrs_access_enabled: false
  kueue_configuration_path: $(ghpc_stage("./kueue-configuration.yaml.tftpl"))

  # The number of nodes to be created
  static_node_count: 2

  # The name of the compute engine reservation in the form of
  # <reservation-name>
  # To target a BLOCK_NAME, the name of the extended reservation
  # can be inputted as <reservation-name>/reservationBlocks/<reservation-block-name>
  reservation:

  accelerator_type: nvidia-h100-mega-80gb
  version_prefix: "1.32."

  enable_periodic_health_checks: false # Make this true to run CHS (healthchecks)
  health_check_schedule: "0 0 * * 0" # Run the health check at 12:00 AM (midnight) every Sunday

  permissions_file_staged_path: $(ghpc_stage("./chs-permissions.yaml.tftpl"))
  chs_output_bucket_name: chs-result
  chs_pvc_claim_name: chs-output-pvc
  chs_cronjob_rendered_path: $(ghpc_stage("./chs-cronjob.yaml.tftpl"))
  chs_pvc_rendered_path: $(ghpc_stage("./chs-pvc.yaml.tftpl"))

deployment_groups:
- group: primary
  modules:
  - id: network1
    source: modules/network/vpc
    settings:
      subnetwork_name: $(vars.deployment_name)-subnet
      ips_per_nat: 6
      mtu: 8244
      secondary_ranges_list:
      - subnetwork_name: $(vars.deployment_name)-subnet
        ranges:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: node_pool_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-np-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectViewer
      - artifactregistry.reader

  - id: workload_service_account
    source: community/modules/project/service-account
    settings:
      name: gke-wl-sa
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - monitoring.viewer
      - stackdriver.resourceMetadata.writer
      - storage.objectAdmin
      - artifactregistry.reader
      - container.admin

  - id: gpunets
    source: modules/network/multivpc
    settings:
      network_name_prefix: $(vars.deployment_name)-gpunet
      global_ip_address_range: 192.169.0.0/16
      network_count: 8
      subnetwork_cidr_suffix: 20 # the subnet can support upto 4k nodes
      mtu: 8244

  - id: gke_cluster
    source: modules/scheduler/gke-cluster
    use: [network1, gpunets, workload_service_account]
    settings:
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      gcp_public_cidrs_access_enabled: $(vars.gcp_public_cidrs_access_enabled)
      configure_workload_identity_sa: true
      enable_gcsfuse_csi: true
      enable_parallelstore_csi: false
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr)  # Allows your machine run kubectl command. It's required for the multi-network setup.
        display_name: "kubectl-access-network"
      k8s_network_names:
        gvnic_prefix: vpc
        gvnic_start_index: 1
      version_prefix: $(vars.version_prefix)
    outputs: [instructions]

  - id: a3_megagpu_pool
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gpunets, node_pool_service_account]
    settings:
      machine_type: a3-megagpu-8g
      static_node_count: $(vars.static_node_count)
      zones: [$(vars.zone)]
      guest_accelerator:
      - type: $(vars.accelerator_type)
        count: 8
      reservation_affinity:
        consume_reservation_type: SPECIFIC_RESERVATION
        specific_reservations:
        - name: $(vars.reservation)
    outputs: [instructions]

  - id: workload_manager_install
    source: modules/management/kubectl-apply
    use: [gke_cluster]
    settings:
      apply_manifests:
      - source: $(vars.permissions_file_staged_path)
        enable: $(vars.enable_periodic_health_checks)
        template_vars:
          project_id: $(vars.project_id)
          deployment_name: $(vars.deployment_name)
      - source: $(vars.chs_pvc_rendered_path)
        enable: $(vars.enable_periodic_health_checks)
        template_vars:
          pvc_name: $(vars.chs_pvc_claim_name)
          access_mode: ReadWriteOnce
          capacity: 1Gi
          storage_class_name: standard-rwo
      - source: $(vars.chs_cronjob_rendered_path)
        enable: $(vars.enable_periodic_health_checks)
        template_vars:
          project_id: $(vars.project_id)
          deployment_name: $(vars.deployment_name)
          region: $(vars.region)
          machine_type: a3-megagpu-8g
          gcs_bucket: $(vars.chs_output_bucket_name)
          gcs_pvc: $(vars.chs_pvc_claim_name)
          cronjob_schedule: $(vars.health_check_schedule)
      kueue:
        install: true
        config_path: $(vars.kueue_configuration_path)
        config_template_vars:
          num_gpus: $(a3_megagpu_pool.static_gpu_count)
          accelerator_type: $(vars.accelerator_type)
      jobset:
        install: true
