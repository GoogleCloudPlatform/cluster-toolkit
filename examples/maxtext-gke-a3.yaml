# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---

blueprint_name: maxtext-a3-mega

vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: maxtext-a3-mega
  region: australia-southeast1
  zone: australia-southeast1-c

  # Cidr block containing the IP of the machine calling terraform.
  # The following line must be updated for this example to work.
  authorized_cidr: <your-ip-address>/32

deployment_groups:
- group: primary
  modules:
  - id: network1
    source: modules/network/vpc
    settings:
      subnetwork_name: maxtext-subnet-a3-mega
      secondary_ranges:
        gke-subnet-a3-mega:
        - range_name: pods
          ip_cidr_range: 10.4.0.0/14
        - range_name: services
          ip_cidr_range: 10.0.32.0/20

  - id: gpunets
    source: modules/network/multivpc
    settings:
      network_name_prefix: $(vars.deployment_name)-gpunet
      global_ip_address_range: 192.169.0.0/16
      network_count: 8
      subnetwork_cidr_suffix: 24

  - id: gke_cluster
    source: modules/scheduler/gke-cluster
    use: [network1, gpunets]
    settings:
      enable_private_endpoint: false  # Allows for access from authorized public IPs
      master_authorized_networks:
      - cidr_block: $(vars.authorized_cidr)  # Allows your machine run kubectl command. It's required for the multi-network setup.
        display_name: "kubectl-access-network"
      #min_master_version: "1.29.6-gke.1326000" # Couldn't find this version in the valid master versions in australia-southeast1-c. Can be left unset to be set by GKE to the version of the most recent official release.
      system_node_pool_machine_type: "e2-standard-32"
    outputs: [instructions]

  - id: group_placement_0
    source: modules/compute/resource-policy
    settings:
      name: gp-a3plus-benchmark-np-0
      group_placement_max_distance: 2

  - id: group_placement_1
    source: modules/compute/resource-policy
    settings:
      name: gp-a3plus-benchmark-np-1
      group_placement_max_distance: 2

  - id: group_placement_2
    source: modules/compute/resource-policy
    settings:
      name: gp-a3plus-benchmark-np-2
      group_placement_max_distance: 2

  - id: group_placement_3
    source: modules/compute/resource-policy
    settings:
      name: gp-a3plus-benchmark-np-3
      group_placement_max_distance: 2

  - id: a3_megagpu_pool_0
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gpunets, group_placement_0]
    settings:
      machine_type: a3-megagpu-8g
      autoscaling_total_min_nodes: 1
      initial_node_count: 20
      zones: [$(vars.zone)]
      host_maintenance_interval: PERIODIC
    outputs: [instructions]

  - id: a3_megagpu_pool_1
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gpunets, group_placement_1]
    settings:
      machine_type: a3-megagpu-8g
      autoscaling_total_min_nodes: 1
      initial_node_count: 20
      zones: [$(vars.zone)]
      host_maintenance_interval: PERIODIC
    outputs: [instructions]

  - id: a3_megagpu_pool_2
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gpunets, group_placement_2]
    settings:
      machine_type: a3-megagpu-8g
      autoscaling_total_min_nodes: 1
      initial_node_count: 20
      zones: [$(vars.zone)]
      host_maintenance_interval: PERIODIC
    outputs: [instructions]

  - id: a3_megagpu_pool_3
    source: modules/compute/gke-node-pool
    use: [gke_cluster, gpunets, group_placement_3]
    settings:
      machine_type: a3-megagpu-8g
      autoscaling_total_min_nodes: 1
      initial_node_count: 20
      zones: [$(vars.zone)]
      host_maintenance_interval: PERIODIC
    outputs: [instructions]

  - id: workload_manager_install
    source: modules/management/kubectl-apply
    use: [gke_cluster]
    settings:
      kueue:
        install: true
      jobset:
        install: true

  - id: topology_aware_scheduler_install
    source: modules/management/kubectl-apply
    use: [gke_cluster]
    settings:
      apply_manifest:
      - source: $(ghpc_stage("maxtext-gke-a3-files"))/gpudirect-tcpxo/topology-scheduler/topology-scheduler-scripts.yaml
      - source: $(ghpc_stage("maxtext-gke-a3-files"))/gpudirect-tcpxo/topology-scheduler/service-account.yaml
      - source: $(ghpc_stage("maxtext-gke-a3-files"))/gpudirect-tcpxo/topology-scheduler/label-nodes-daemon.yaml
      - source: $(ghpc_stage("maxtext-gke-a3-files"))/gpudirect-tcpxo/topology-scheduler/schedule-daemon.yaml

  - id: workload_manager_config
    source: modules/management/kubectl-apply
    use: [gke_cluster]
    settings:
      apply_manifest:
      - source: $(ghpc_stage("maxtext-gke-a3-files"))/config-map.yaml.tftpl
        template_vars: {config_map: "a3plus-benchmark-resources-configmap", num_nodes: "80"}
      - source: $(ghpc_stage("maxtext-gke-a3-files"))/kueue-credentials.yaml.tftpl
        template_vars: {num_chips: "640"}
