# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
blueprint_name: slurm-a3-cluster

terraform_backend_defaults:
  type: gcs
  configuration:
    bucket: customer-tf-state-bucket  # modify to be a bucket owned and writable by customer

vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: slurm-a3-cluster
  region: customer-region
  zone: customer-zone
  server_ip_homefs: 0.0.0.0 ## MUST set to IP address of Filestore instance from base deployment!
  remote_mount_homefs: /nfsshare
  local_mount_homefs: /home
  zones:
  - $(vars.zone)
  disk_size_gb: 200
  final_image_family: slurm-dlvm
  slurm_cluster_name: slurm0
  enable_reconfigure: true
  enable_cleanup_compute: true
  enable_cleanup_subscriptions: true
  a3_partition_name: a3
  a3_static_cluster_size: 32
  # a3_reservation_name must be specified; if Google staff have provided you
  # with a reservation name, use it. Otherwise supply user-created reservation.
  a3_reservation_name: a3-reservation-0
  # a3_maintenance_interval should be empty string by default; if Google staff
  # have created a reservation, they will also provide a3_maintenance_interval
  a3_maintenance_interval: ""
  # network parameters must match base blueprint deployment_name!
  # these values are accurate if deployment_name was not modified from example
  network_name_system: slurm-a3-base-sysnet
  subnetwork_name_system: slurm-a3-base-sysnet-subnet

deployment_groups:
- group: cluster
  modules:
  - id: sysnet
    source: modules/network/pre-existing-vpc
    settings:
      network_name: $(vars.network_name_system)
      subnetwork_name: $(vars.subnetwork_name_system)

  - id: gpunets
    source: modules/network/multivpc
    settings:
      global_ip_address_range: 10.0.0.0/9
      network_name_prefix: $(vars.deployment_name)-gpunet
      network_count: 4
      subnetwork_cidr_suffix: 20

  - id: homefs
    source: modules/file-system/pre-existing-network-storage
    settings:
      server_ip: $(vars.server_ip_homefs)
      remote_mount: $(vars.remote_mount_homefs)
      local_mount: $(vars.local_mount_homefs)

  - id: compute_sa
    source: community/modules/project/service-account
    settings:
      name: compute
      project_roles:
      - logging.logWriter
      - monitoring.metricWriter
      - pubsub.subscriber
      - storage.objectAdmin

  - id: debug_node_group
    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
    settings:
      node_count_static: 0
      node_count_dynamic_max: 4
      machine_type: n2-standard-2
      instance_image_custom: true
      instance_image:
        family: $(vars.final_image_family)
        project: $(vars.project_id)

  - id: debug_partition
    source: community/modules/compute/schedmd-slurm-gcp-v5-partition
    use:
    - debug_node_group
    - sysnet
    - homefs
    settings:
      partition_name: debug
      exclusive: false
      enable_placement: false

  - id: a3_node_group
    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
    use:
    - gpunets
    settings:
      reservation_name: $(vars.a3_reservation_name)
      maintenance_interval: $(vars.a3_maintenance_interval)
      node_count_static: $(vars.a3_static_cluster_size)
      node_count_dynamic_max: 0
      disk_type: pd-ssd
      machine_type: a3-highgpu-8g
      instance_image_custom: true
      disable_public_ips: true
      enable_smt: true
      instance_image:
        family: $(vars.final_image_family)
        project: $(vars.project_id)
      node_conf:
        CoresPerSocket: 52
        ThreadsPerCore: 2
      on_host_maintenance: TERMINATE
      service_account:
        email: $(compute_sa.service_account_email)
        scopes:
        - cloud-platform
      bandwidth_tier: gvnic_enabled

  - id: a3_partition
    source: community/modules/compute/schedmd-slurm-gcp-v5-partition
    use:
    - a3_node_group
    - sysnet
    - homefs
    settings:
      partition_name: $(vars.a3_partition_name)
      enable_placement: false
      exclusive: false
      is_default: true
      partition_conf:
        OverSubscribe: EXCLUSIVE

  - id: controller_startup
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: stage_scripts.sh
        content: |
          #!/bin/bash
          # use script from master branch which is actively maintained
          curl -s --create-dirs -o /opt/apps/adm/slurm/scripts/receive-data-path-manager \
              https://raw.githubusercontent.com/GoogleCloudPlatform/slurm-gcp/master/tools/prologs-epilogs/receive-data-path-manager
          chmod 0755 /opt/apps/adm/slurm/scripts/receive-data-path-manager
          mkdir -p /opt/apps/adm/slurm/partition-$(vars.a3_partition_name)-prolog_slurmd.d
          mkdir -p /opt/apps/adm/slurm/partition-$(vars.a3_partition_name)-epilog_slurmd.d
          ln -s /opt/apps/adm/slurm/scripts/receive-data-path-manager /opt/apps/adm/slurm/partition-$(vars.a3_partition_name)-prolog_slurmd.d/start-rxdm.prolog_slurmd
          ln -s /opt/apps/adm/slurm/scripts/receive-data-path-manager /opt/apps/adm/slurm/partition-$(vars.a3_partition_name)-epilog_slurmd.d/stop-rxdm.epilog_slurmd
      - type: shell
        destination: reset_enroot.sh
        content: |
          #!/bin/bash
          # reset enroot to defaults of files under /home and running under /run
          # allows basic enroot testing on login/controller nodes (reduced I/O)
          rm -f /etc/enroot/enroot.conf

  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v5-controller
    use:
    - sysnet
    - a3_partition
    - debug_partition
    - homefs
    settings:
      machine_type: c2-standard-8
      cloud_parameters:
        resume_rate: 0
        resume_timeout: 900
        suspend_rate: 0
        suspend_timeout: 600
        no_comma_params: false
        tree_width: $(vars.a3_static_cluster_size)
      instance_image_custom: true
      instance_image:
        family: $(vars.final_image_family)
        project: $(vars.project_id)
      slurm_conf_tpl: modules/embedded/community/modules/scheduler/schedmd-slurm-gcp-v5-controller/etc/long-prolog-slurm.conf.tpl
      controller_startup_script: $(controller_startup.startup_script)
      enable_external_prolog_epilog: true

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v5-login
    use:
    - sysnet
    - slurm_controller
    settings:
      disk_type: pd-balanced
      instance_image_custom: true
      instance_image:
        family: $(vars.final_image_family)
        project: $(vars.project_id)
      machine_type: c2-standard-4
      startup_script: |
        #!/bin/bash
        # reset enroot to defaults of files under /home and running under /run
        # allows basic enroot testing on login node (reduced I/O)
        rm -f /etc/enroot/enroot.conf
