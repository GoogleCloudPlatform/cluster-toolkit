# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
blueprint_name: a3mega-cluster

# this blueprint should be used with the extra variables defined in
# deployment-image-cluster.yaml
vars:
  deployment_name: a3mega-cluster
  a3mega_partition_name: a3mega
  enable_placement: false
  remote_mount_homefs: /nfsshare
  local_mount_homefs: /home
  instance_image_custom: true
  instance_image:
    family: $(vars.final_image_family)
    project: $(vars.project_id)

deployment_groups:
- group: cluster
  modules:
  - id: sysnet
    source: modules/network/pre-existing-vpc
    settings:
      network_name: $(vars.network_name_system)
      subnetwork_name: $(vars.subnetwork_name_system)

  # if an existing bucket is desired, follow modules/file-system/pre-existing-network-storage/README.md
  - id: data-bucket
    source: community/modules/file-system/cloud-storage-bucket
    settings:
      local_mount: /gcs
      mount_options: defaults,rw,_netdev,implicit_dirs,allow_other,implicit_dirs,file_mode=777,dir_mode=777
      random_suffix: true

  - id: gpunets
    source: modules/network/multivpc
    settings:
      network_name_prefix: $(vars.deployment_name)-gpunet
      global_ip_address_range: 10.0.0.0/9
      network_count: 8
      subnetwork_cidr_suffix: 20

  - id: homefs
    source: modules/file-system/pre-existing-network-storage
    settings:
      server_ip: $(vars.server_ip_homefs)
      remote_mount: $(vars.remote_mount_homefs)
      local_mount: $(vars.local_mount_homefs)
      mount_options: "defaults,hard"

  - id: debug_nodeset
    use: [sysnet]
    source: community/modules/compute/schedmd-slurm-gcp-v6-nodeset
    settings:
      node_count_static: 0
      node_count_dynamic_max: 4
      machine_type: n2-standard-2

  - id: debug_partition
    source: community/modules/compute/schedmd-slurm-gcp-v6-partition
    use:
    - debug_nodeset
    settings:
      partition_name: debug
      exclusive: false

  - id: a3mega_startup
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: setup_aperture.sh
        content: |
          #!/bin/bash
          MAX_APERTURE_RETRIES=10
          aperture_retries=0

          until lspci -nn -D | grep -q '1ae0:0084' || [ $aperture_retries -eq $MAX_APERTURE_RETRIES ]; do
            echo "\$(date): APERTURE devices not yet fully available (attempt $aperture_retries/$MAX_APERTURE_RETRIES). Retrying in 5 seconds..."
            sleep 5
            aperture_retries=\$((aperture_retries + 1))
          done

          if [ $aperture_retries -eq $MAX_APERTURE_RETRIES ]; then
            echo "\$(date): ERROR: APERTURE devices failed to initialize after $MAX_APERTURE_RETRIES retries."
            # Consider taking additional error-handling actions
            exit 0
          fi

          echo "\$(date): Aperture devices ready!"
          lspci -nn -D | grep '1ae0:0084' | awk '{print $1}' | xargs --no-run-if-empty -I {} -n 1 mkdir -p /dev/aperture_devices/{}
          lspci -nn -D | grep '1ae0:0084' | awk '{print $1}' | xargs --no-run-if-empty -I {} -n 1 umount -f /dev/aperture_devices/{} || true
          lspci -nn -D | grep '1ae0:0084' | awk '{print $1}' | xargs --no-run-if-empty -I {} -n 1 mount --bind /sys/bus/pci/devices/{} /dev/aperture_devices/{}
          if [ -d /dev/aperture_devices ]; then
              chmod -R a+r /dev/aperture_devices/
              chmod a+rw /dev/aperture_devices/*/resource*
          fi
          echo "\$(date): Aperture devices mounted!"
      - type: ansible-local
        destination: enable_dcgm.yml
        content: |
          ---
          - name: Enable NVIDIA DCGM on GPU nodes
            hosts: all
            become: true
            vars:
              enable_ops_agent: $(vars.enable_ops_agent)
              enable_nvidia_dcgm: $(vars.enable_nvidia_dcgm)
            tasks:
            - name: Update Ops Agent configuration
              ansible.builtin.blockinfile:
                path: /etc/google-cloud-ops-agent/config.yaml
                insertafter: EOF
                block: |
                  metrics:
                    receivers:
                      dcgm:
                        type: dcgm
                    service:
                      pipelines:
                        dcgm:
                          receivers:
                            - dcgm
              notify:
              - Restart Google Cloud Ops Agent
            handlers:
            - name: Restart Google Cloud Ops Agent
              ansible.builtin.service:
                name: google-cloud-ops-agent.service
                state: "{{ 'restarted' if enable_ops_agent else 'stopped' }}"
                enabled: "{{ enable_ops_agent }}"
            post_tasks:
            - name: Enable Google Cloud Ops Agent
              ansible.builtin.service:
                name: google-cloud-ops-agent.service
                state: "{{ 'started' if enable_ops_agent else 'stopped' }}"
                enabled: "{{ enable_ops_agent }}"
            - name: Enable NVIDIA DCGM
              ansible.builtin.service:
                name: nvidia-dcgm.service
                state: "{{ 'started' if enable_nvidia_dcgm else 'stopped' }}"
                enabled: "{{ enable_nvidia_dcgm }}"

  - id: a3mega_nodeset
    source: community/modules/compute/schedmd-slurm-gcp-v6-nodeset
    use:
    - sysnet
    - gpunets
    settings:
      node_count_static: $(vars.a3mega_cluster_size)
      node_count_dynamic_max: 0
      disk_type: pd-ssd
      machine_type: a3-megagpu-8g
      enable_public_ips: false
      enable_smt: true
      node_conf:
        CoresPerSocket: 52
        ThreadsPerCore: 2
      on_host_maintenance: TERMINATE
      bandwidth_tier: gvnic_enabled
      reservation_name: $(vars.a3mega_reservation_name)
      maintenance_interval: $(vars.a3mega_maintenance_interval)
      startup_script: $(a3mega_startup.startup_script)

  - id: a3mega_partition
    source: community/modules/compute/schedmd-slurm-gcp-v6-partition
    use:
    - a3mega_nodeset
    settings:
      partition_name: $(vars.a3mega_partition_name)
      exclusive: false
      is_default: true
      partition_conf:
        OverSubscribe: EXCLUSIVE
        ResumeTimeout: 900
        SuspendTimeout: 600

  - id: controller_startup
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: stage_scripts.sh
        content: |
          #!/bin/bash
          SLURM_ROOT=/opt/apps/adm/slurm
          mkdir -m 0755 -p "${SLURM_ROOT}/scripts"
          mkdir -p "${SLURM_ROOT}/partition-$(vars.a3mega_partition_name)-prolog_slurmd.d"
          mkdir -p "${SLURM_ROOT}/partition-$(vars.a3mega_partition_name)-epilog_slurmd.d"
          curl -s -o "${SLURM_ROOT}/scripts/sudo-oslogin" \
              https://raw.githubusercontent.com/GoogleCloudPlatform/slurm-gcp/master/tools/prologs-epilogs/sudo-oslogin
          chmod 0755 "${SLURM_ROOT}/scripts/sudo-oslogin"
          ln -s "${SLURM_ROOT}/scripts/sudo-oslogin" "${SLURM_ROOT}/partition-$(vars.a3mega_partition_name)-prolog_slurmd.d/sudo-oslogin.prolog_slurmd"
          ln -s "${SLURM_ROOT}/scripts/sudo-oslogin" "${SLURM_ROOT}/partition-$(vars.a3mega_partition_name)-epilog_slurmd.d/sudo-oslogin.epilog_slurmd"
          curl -s -o "${SLURM_ROOT}/scripts/rxdm" \
              https://raw.githubusercontent.com/GoogleCloudPlatform/slurm-gcp/master/tools/prologs-epilogs/receive-data-path-manager-mega
          chmod 0755 "${SLURM_ROOT}/scripts/rxdm"
          ln -s "${SLURM_ROOT}/scripts/rxdm" "${SLURM_ROOT}/partition-$(vars.a3mega_partition_name)-prolog_slurmd.d/rxdm.prolog_slurmd"
          ln -s "${SLURM_ROOT}/scripts/rxdm" "${SLURM_ROOT}/partition-$(vars.a3mega_partition_name)-epilog_slurmd.d/rxdm.epilog_slurmd"
      - type: shell
        destination: reset_enroot.sh
        content: |
          #!/bin/bash
          # reset enroot to defaults of files under /home and running under /run
          # allows basic enroot testing with reduced I/O performance
          rm -f /etc/enroot/enroot.conf

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-login
    use:
    - sysnet
    settings:
      name_prefix: login
      disk_type: pd-balanced
      machine_type: c2-standard-4
      enable_login_public_ips: false

  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-controller
    use:
    - sysnet
    - a3mega_partition
    - debug_partition
    - slurm_login
    - homefs
    - data-bucket
    settings:
      machine_type: c2-standard-8
      enable_cleanup_compute: true
      enable_external_prolog_epilog: true
      slurm_conf_tpl: modules/embedded/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/etc/long-prolog-slurm.conf.tpl
      enable_controller_public_ips: false
      controller_startup_script: $(controller_startup.startup_script)
      login_startup_script: |
        #!/bin/bash
        # reset enroot to defaults of files under /home and running under /run
        # allows basic enroot testing with reduced I/O performance
        rm -f /etc/enroot/enroot.conf
      prolog_scripts:
      - filename: set_hostname_for_enroot.sh
        content: |
          #!/bin/bash
          hostname | tee /etc/hostname
